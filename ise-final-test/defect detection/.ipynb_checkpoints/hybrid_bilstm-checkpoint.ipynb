{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93efcad6",
   "metadata": {},
   "source": [
    "## ISE: Defect Detection Challenge\n",
    "#### Description\n",
    "In this competition, your task is to develop a model that can accurately classify source code snippets as either secure or insecure. With the rise of software vulnerabilities like resource leaks, use-after-free vulnerabilities, and denial-of-service (DoS) attacks, identifying insecure code is crucial for maintaining robust software systems.\n",
    "Participants will be provided with a dataset containing labeled code snippets. The labels indicate whether the code is secure (0) or insecure (1). Your goal is to create an effective machine learning model that can predict these labels with high accuracy.\n",
    "#### Key Objectives \n",
    "- Analyze code snippets for potential vulnerabilities.\n",
    "- Develop models to automate the classification of secure and insecure code.\n",
    "- Ensure the ROC score exceeds 0.63."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d0c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 00:10:33.745672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-06 00:10:33.893613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754413833.946675  122191 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754413833.962350  122191 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754413834.079953  122191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754413834.079982  122191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754413834.079984  122191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754413834.079985  122191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-06 00:10:34.096716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98751e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (20000, 3)\n",
      "Test data shape: (7000, 2)\n",
      "\n",
      "Training data columns: ['ID', 'code', 'Label']\n",
      "   ID                                               code  Label\n",
      "0   0  int page_check_range(target_ulong start, targe...      0\n",
      "1   1  static void pxa2xx_lcdc_dma0_redraw_rot0(PXA2x...      0\n",
      "2   2  void OPPROTO op_POWER_slq (void)\\n\\n{\\n\\n    u...      1\n",
      "ID       0\n",
      "code     0\n",
      "Label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# data info\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
    "\n",
    "print(train_df.head(3))\n",
    "print(train_df.isnull().sum())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509e62c",
   "metadata": {},
   "source": [
    "## C++ Code Preprocessing Pipeline \n",
    "- Basic Text Cleaning \n",
    "- C++ Specific Normalization\n",
    "- Features Enginerring \n",
    "- Tokenization\n",
    "- Vectorization using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc246a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_cpp_code(code):\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "    \n",
    "    # Remove multi-line comments  \n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove string literals \n",
    "    code = re.sub(r'\"[^\"]*\"', '\"STRING\"', code)\n",
    "    code = re.sub(r\"'[^']*'\", \"'CHAR'\", code)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    code = re.sub(r'\\s+', ' ', code)\n",
    "    code = code.strip()\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cpp_code(code):\n",
    "    # Normalize variable \n",
    "    code = re.sub(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', \n",
    "                  lambda m: normalize_identifier(m.group()), code)\n",
    "    \n",
    "    # Normalize numeric \n",
    "    code = re.sub(r'\\b\\d+\\b', 'NUM', code)\n",
    "    code = re.sub(r'\\b0x[0-9a-fA-F]+\\b', 'HEX', code)\n",
    "    \n",
    "    # Normalize function calls \n",
    "    code = re.sub(r'(\\w+)\\s*\\(', r'FUNC(', code)\n",
    "    \n",
    "    return code\n",
    "\n",
    "def normalize_identifier(name):\n",
    "    # Keep important C++ keywords and functions\n",
    "    cpp_keywords = {'int', 'char', 'void', 'if', 'else', 'for', 'while', \n",
    "                   'malloc', 'free', 'strcpy', 'strlen', 'memcpy', 'sizeof'}\n",
    "    \n",
    "    if name.lower() in cpp_keywords:\n",
    "        return name\n",
    "    elif len(name) <= 3:\n",
    "        return name  # Keep short vars\n",
    "    else:\n",
    "        return 'VAR'  # Generic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_security_features(code):\n",
    "    features = {}\n",
    "    \n",
    "    dangerous_funcs = ['strcpy', 'strcat', 'gets', 'sprintf', 'scanf', \n",
    "                      'malloc', 'free', 'memcpy', 'strncpy', 'strncat',\n",
    "                      'vsprintf', 'vsnprintf', 'sscanf', 'fscanf', 'fgets',\n",
    "                      'alloca', 'realloc', 'calloc']\n",
    "    \n",
    "    for func in dangerous_funcs:\n",
    "        features[f'has_{func}'] = int(func in code.lower())\n",
    "    \n",
    "    # Memory operations \n",
    "    features['ptr_operations'] = len(re.findall(r'\\*|\\&', code))\n",
    "    features['array_access'] = len(re.findall(r'\\[.*?\\]', code))\n",
    "    features['memory_alloc'] = len(re.findall(r'malloc|calloc|new|alloca|realloc', code, re.IGNORECASE))\n",
    "    features['memory_free'] = len(re.findall(r'free|delete', code, re.IGNORECASE))\n",
    "    \n",
    "    # Advanced vulnerability patterns\n",
    "    features['buffer_overflow_risk'] = len(re.findall(r'(strcpy|strcat|gets|sprintf)\\s*\\(', code, re.IGNORECASE))\n",
    "    features['format_string_vuln'] = len(re.findall(r'printf\\s*\\(\\s*[a-zA-Z_]\\w*\\s*[,)]', code))\n",
    "    features['use_after_free_risk'] = detect_use_after_free_pattern(code)\n",
    "    features['double_free_risk'] = detect_double_free_pattern(code)\n",
    "    features['memory_leak_risk'] = abs(features['memory_alloc'] - features['memory_free'])\n",
    "    \n",
    "    # Input validation issues\n",
    "    features['unchecked_input'] = len(re.findall(r'(scanf|gets|fgets)\\s*\\(', code, re.IGNORECASE))\n",
    "    features['missing_null_check'] = detect_missing_null_checks(code)\n",
    "    features['array_bounds_risk'] = detect_array_bounds_issues(code)\n",
    "    \n",
    "    # Integer overflow/underflow risks\n",
    "    features['integer_overflow_risk'] = len(re.findall(r'(\\+\\+|\\-\\-|\\+=|\\-=|\\*=).*?(\\[|\\*)', code))\n",
    "    features['signed_unsigned_mix'] = len(re.findall(r'(unsigned|signed)\\s+\\w+.*?(signed|unsigned)', code, re.IGNORECASE))\n",
    "    \n",
    "    # Control flow complexity (enhanced)\n",
    "    features['if_statements'] = len(re.findall(r'\\bif\\b', code))\n",
    "    features['nested_loops'] = detect_nested_complexity(code, r'\\b(for|while)\\b')\n",
    "    features['switch_statements'] = len(re.findall(r'\\bswitch\\b', code))\n",
    "    features['goto_statements'] = len(re.findall(r'\\bgoto\\b', code))\n",
    "    features['function_calls'] = len(re.findall(r'\\w+\\s*\\(', code))\n",
    "    \n",
    "    # Code quality indicators\n",
    "    features['magic_numbers'] = len(re.findall(r'\\b\\d{2,}\\b', code))\n",
    "    features['long_functions'] = int(len(code.split('\\n')) > 50)\n",
    "    features['deep_nesting'] = calculate_max_nesting_depth(code)\n",
    "    features['cyclomatic_complexity'] = estimate_cyclomatic_complexity(code)\n",
    "    \n",
    "    # String and file operations\n",
    "    features['string_operations'] = len(re.findall(r'str(cpy|cat|cmp|len|chr|str)', code, re.IGNORECASE))\n",
    "    features['file_operations'] = len(re.findall(r'(fopen|fclose|fread|fwrite|fprintf)', code, re.IGNORECASE))\n",
    "    \n",
    "    # Pointer arithmetic and casting\n",
    "    features['pointer_arithmetic'] = len(re.findall(r'(\\*\\s*\\w+\\s*[\\+\\-]|\\w+\\s*[\\+\\-]\\s*\\d+\\s*\\))', code))\n",
    "    features['type_casting'] = len(re.findall(r'\\([a-zA-Z_]\\w*\\s*\\*?\\s*\\)', code))\n",
    "    features['void_pointer_usage'] = len(re.findall(r'void\\s*\\*', code, re.IGNORECASE))\n",
    "    \n",
    "    # Security-specific patterns\n",
    "    features['hardcoded_values'] = detect_hardcoded_credentials(code)\n",
    "    features['privilege_operations'] = len(re.findall(r'(setuid|setgid|chmod|chown|su|sudo)', code, re.IGNORECASE))\n",
    "    features['system_calls'] = len(re.findall(r'(system|exec|popen|fork)', code, re.IGNORECASE))\n",
    "    \n",
    "    # Statistical features\n",
    "    features['code_length'] = len(code)\n",
    "    features['line_count'] = len(code.split('\\n'))\n",
    "    features['avg_line_length'] = features['code_length'] / max(1, features['line_count'])\n",
    "    features['char_entropy'] = calculate_entropy(code)\n",
    "    features['unique_char_ratio'] = len(set(code.lower())) / max(1, len(code))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefae8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_use_after_free_pattern(code):\n",
    "    \"\"\"Detect potential use-after-free patterns\"\"\"\n",
    "    patterns = [\n",
    "        r'free\\s*\\([^)]+\\).*?\\*\\s*\\w+',  # free followed by dereference\n",
    "        r'delete\\s+\\w+.*?\\w+\\s*\\[',      # delete followed by array access\n",
    "        r'free\\s*\\([^)]+\\).*?\\w+\\s*\\(',  # free followed by function call with same var\n",
    "    ]\n",
    "    \n",
    "    count = 0\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, code, re.DOTALL | re.IGNORECASE)\n",
    "        count += len(matches)\n",
    "    \n",
    "    return count\n",
    "\n",
    "def detect_double_free_pattern(code):\n",
    "    \"\"\"Detect potential double free patterns\"\"\"\n",
    "    free_calls = re.findall(r'free\\s*\\(\\s*(\\w+)\\s*\\)', code, re.IGNORECASE)\n",
    "    if len(free_calls) != len(set(free_calls)):\n",
    "        return 1  # Potential double free\n",
    "    return 0\n",
    "\n",
    "def detect_missing_null_checks(code):\n",
    "    \"\"\"Detect pointer usage without null checks\"\"\"\n",
    "    ptr_usage = len(re.findall(r'\\*\\s*\\w+', code))\n",
    "    null_checks = len(re.findall(r'if\\s*\\(\\s*\\w+\\s*[!=]=\\s*NULL\\s*\\)', code, re.IGNORECASE))\n",
    "    return max(0, ptr_usage - null_checks)\n",
    "\n",
    "def detect_array_bounds_issues(code):\n",
    "    \"\"\"Detect array access without bounds checking\"\"\"\n",
    "    array_access = re.findall(r'\\w+\\s*\\[\\s*([^]]+)\\s*\\]', code)\n",
    "    bounds_checks = len(re.findall(r'if\\s*\\([^)]*(<|>|<=|>=)[^)]*\\)', code))\n",
    "    return max(0, len(array_access) - bounds_checks)\n",
    "\n",
    "def detect_nested_complexity(code, pattern):\n",
    "    \"\"\"Detect nested control structures\"\"\"\n",
    "    lines = code.split('\\n')\n",
    "    max_nested = 0\n",
    "    current_nested = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if re.search(pattern, line):\n",
    "            current_nested += 1\n",
    "            max_nested = max(max_nested, current_nested)\n",
    "        if '}' in line:\n",
    "            current_nested = max(0, current_nested - 1)\n",
    "    \n",
    "    return max_nested\n",
    "\n",
    "def calculate_max_nesting_depth(code):\n",
    "    \"\"\"Calculate maximum nesting depth\"\"\"\n",
    "    depth = 0\n",
    "    max_depth = 0\n",
    "    \n",
    "    for char in code:\n",
    "        if char == '{':\n",
    "            depth += 1\n",
    "            max_depth = max(max_depth, depth)\n",
    "        elif char == '}':\n",
    "            depth = max(0, depth - 1)\n",
    "    \n",
    "    return max_depth\n",
    "\n",
    "def estimate_cyclomatic_complexity(code):\n",
    "    \"\"\"Estimate cyclomatic complexity\"\"\"\n",
    "    decision_points = ['if', 'else', 'elif', 'for', 'while', 'case', 'catch', '\\?', '&&', '\\|\\|']\n",
    "    complexity = 1  # Base complexity\n",
    "    \n",
    "    for keyword in decision_points:\n",
    "        complexity += len(re.findall(rf'\\b{keyword}\\b', code, re.IGNORECASE))\n",
    "    \n",
    "    return complexity\n",
    "\n",
    "def detect_hardcoded_credentials(code):\n",
    "    \"\"\"Detect hardcoded passwords, keys, etc.\"\"\"\n",
    "    patterns = [\n",
    "        r'(password|passwd|pwd)\\s*=\\s*[\"\\'][^\"\\']{3,}[\"\\']',\n",
    "        r'(key|secret|token)\\s*=\\s*[\"\\'][^\"\\']{8,}[\"\\']',\n",
    "        r'(api_key|apikey)\\s*=\\s*[\"\\'][^\"\\']{10,}[\"\\']',\n",
    "    ]\n",
    "    \n",
    "    count = 0\n",
    "    for pattern in patterns:\n",
    "        count += len(re.findall(pattern, code, re.IGNORECASE))\n",
    "    \n",
    "    return count\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"Calculate Shannon entropy of text\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    char_counts = {}\n",
    "    for char in text.lower():\n",
    "        char_counts[char] = char_counts.get(char, 0) + 1\n",
    "    \n",
    "    entropy = 0\n",
    "    text_len = len(text)\n",
    "    \n",
    "    for count in char_counts.values():\n",
    "        probability = count / text_len\n",
    "        if probability > 0:\n",
    "            entropy -= probability * np.log2(probability)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256450b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cpp_dataset(df):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for C++ code dataset with enhanced features\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    print(f\"Processing {len(df)} code samples...\")\n",
    "    \n",
    "    print(\"Cleaning code...\")\n",
    "    processed_df['cleaned_code'] = processed_df['code'].apply(clean_cpp_code)\n",
    "    \n",
    "    print(\"Step 2: Normalizing code...\")\n",
    "    processed_df['normalized_code'] = processed_df['cleaned_code'].apply(normalize_cpp_code)\n",
    "    \n",
    "    print(\"Step 3: Extracting enhanced security features...\")\n",
    "    security_features_list = []\n",
    "    \n",
    "    for idx, code in enumerate(processed_df['code']):\n",
    "        if idx % 5000 == 0:\n",
    "            print(f\"  Processing sample {idx}/{len(processed_df)}\")\n",
    "        \n",
    "        features = extract_security_features(code) \n",
    "        security_features_list.append(features)\n",
    "    \n",
    "    security_df = pd.DataFrame(security_features_list)\n",
    "    \n",
    "    result_df = pd.concat([\n",
    "        processed_df[['normalized_code']],  \n",
    "        security_df,  \n",
    "    ], axis=1)\n",
    "    \n",
    "    if 'Label' in processed_df.columns:\n",
    "        result_df['Label'] = processed_df['Label']\n",
    "    \n",
    "    print(f\"Preprocessing complete!\")\n",
    "    print(f\"Enhanced features created: {len(security_df.columns)} features\")\n",
    "    print(f\"Final shape: {result_df.shape}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54837cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "def augment_features(X, y, noise_factor=0.1, augment_ratio=0.5):\n",
    "    \"\"\"Add gaussian noise to numerical features\"\"\"\n",
    "    n_samples = int(len(X) * augment_ratio)\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    \n",
    "    X_aug = X[indices].copy()\n",
    "    y_aug = y[indices].copy()\n",
    "    \n",
    "    tfidf_end = 2000  \n",
    "    X_aug[:, tfidf_end:] += np.random.normal(0, noise_factor, X_aug[:, tfidf_end:].shape)\n",
    "    \n",
    "    X_combined = np.vstack([X, X_aug])\n",
    "    y_combined = np.hstack([y, y_aug])\n",
    "    \n",
    "    shuffle_idx = np.random.permutation(len(X_combined))\n",
    "    return X_combined[shuffle_idx], y_combined[shuffle_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d898313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced features...\n",
      "Step 1: Applying enhanced preprocessing...\n",
      "Processing 20000 code samples...\n",
      "Step 1: Cleaning code...\n",
      "Step 2: Normalizing code...\n",
      "Step 3: Extracting enhanced security features...\n",
      "  Processing sample 0/20000\n",
      "  Processing sample 5000/20000\n",
      "  Processing sample 10000/20000\n",
      "  Processing sample 15000/20000\n",
      "Preprocessing complete!\n",
      "Enhanced features created: 54 features\n",
      "Final shape: (20000, 56)\n",
      "Processing 7000 code samples...\n",
      "Step 1: Cleaning code...\n",
      "Step 2: Normalizing code...\n",
      "Step 3: Extracting enhanced security features...\n",
      "  Processing sample 0/7000\n",
      "  Processing sample 5000/7000\n",
      "Preprocessing complete!\n",
      "Enhanced features created: 54 features\n",
      "Final shape: (7000, 55)\n",
      "Step 2: Creating TF-IDF features...\n",
      "Step 3: Using enhanced security features...\n",
      "Total enhanced features: 54\n",
      "Feature categories:\n",
      "  - Dangerous function detection: 18\n",
      "  - Vulnerability patterns: 7\n",
      "  - Code complexity: 2\n",
      "  - Security patterns: 3\n",
      "Final shapes - Train: (20000, 4054), Test: (7000, 4054)\n",
      "TF-IDF features: 4000\n",
      "Enhanced security features: 54\n",
      "Total features: 4054\n",
      "\n",
      "Class distribution:\n",
      "  Class 0: 10878 samples (54.4%)\n",
      "  Class 1: 9122 samples (45.6%)\n"
     ]
    }
   ],
   "source": [
    "def create_pipeline_features(train_df, test_df):\n",
    "    \"\"\"Enhanced pipeline using your existing preprocessing with advanced features\"\"\"\n",
    "    \n",
    "    print(\"Step 1: Applying enhanced preprocessing...\")\n",
    "\n",
    "    train_processed = preprocess_cpp_dataset(train_df.copy())\n",
    "    test_processed = preprocess_cpp_dataset(test_df.copy())\n",
    "    \n",
    "    print(\"Step 2: Creating TF-IDF features...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=4000,  \n",
    "        ngram_range=(1, 3),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "        stop_words='english'  \n",
    "    )\n",
    "    \n",
    "    tfidf_train = vectorizer.fit_transform(train_processed['normalized_code'])\n",
    "    tfidf_test = vectorizer.transform(test_processed['normalized_code'])\n",
    "    \n",
    "    print(\"Step 3: Using enhanced security features...\")\n",
    "\n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                   if col not in ['normalized_code', 'Label']]\n",
    "    \n",
    "    print(f\"Total enhanced features: {len(feature_cols)}\")\n",
    "    print(\"Feature categories:\")\n",
    "    print(f\"  - Dangerous function detection: {len([f for f in feature_cols if f.startswith('has_')])}\")\n",
    "    print(f\"  - Vulnerability patterns: {len([f for f in feature_cols if 'risk' in f or 'vuln' in f])}\")\n",
    "    print(f\"  - Code complexity: {len([f for f in feature_cols if any(x in f for x in ['complexity', 'nesting', 'depth'])])}\")\n",
    "    print(f\"  - Security patterns: {len([f for f in feature_cols if any(x in f for x in ['hardcoded', 'privilege', 'system'])])}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_numerical = scaler.fit_transform(train_processed[feature_cols].fillna(0))\n",
    "    test_numerical = scaler.transform(test_processed[feature_cols].fillna(0))\n",
    "    \n",
    "    X_train = np.hstack([\n",
    "        tfidf_train.toarray(),\n",
    "        train_numerical\n",
    "    ])\n",
    "    \n",
    "    X_test = np.hstack([\n",
    "        tfidf_test.toarray(),\n",
    "        test_numerical\n",
    "    ])\n",
    "    \n",
    "    y_train = train_processed['Label'].values\n",
    "    \n",
    "    print(f\"Final shapes - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"TF-IDF features: {tfidf_train.shape[1]}\")\n",
    "    print(f\"Enhanced security features: {len(feature_cols)}\")\n",
    "    print(f\"Total features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "print(\"Creating enhanced features...\")\n",
    "X_train, X_test, y_train = create_pipeline_features(train_df, test_df)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count} samples ({count/len(y_train)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940cafe-f784-41b6-9e2a-277087ef5d87",
   "metadata": {},
   "source": [
    "## Prepare Data for Hybrid Model Training\n",
    "- Create code sequences \n",
    "- Prepare hybrid data: sequences + security features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9708e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codebert_token_embeddings(tokenizer, vocab_size):\n",
    "    \"\"\"PyTorch CodeBERT version - future-proof\"\"\"\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"Loading CodeBERT embeddings with PyTorch...\")\n",
    "    \n",
    "    codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "    codebert_model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "    codebert_model.eval()\n",
    "    \n",
    "    codebert_embeddings = codebert_model.embeddings.word_embeddings.weight.data.numpy()\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, 768))\n",
    "    \n",
    "    for i, (word_index, word) in enumerate(tokenizer.index_word.items()):\n",
    "        if word_index >= vocab_size:\n",
    "            break\n",
    "            \n",
    "        # Try to find token in CodeBERT vocabulary\n",
    "        if word in codebert_tokenizer.vocab:\n",
    "            codebert_token_id = codebert_tokenizer.vocab[word]\n",
    "            embedding_matrix[word_index] = codebert_embeddings[codebert_token_id]\n",
    "        else:\n",
    "            # For out-of-vocabulary tokens, use random initialization\n",
    "            embedding_matrix[word_index] = np.random.normal(0, 0.1, 768)\n",
    "    \n",
    "    print(f\"Mapped {vocab_size} tokens to CodeBERT embeddings\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99054e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating code sequences...\n",
      "Vocabulary size: 39678\n",
      "Sequence shape: (20000, 512)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def create_code_sequences(train_df, test_df, max_features=10000, max_len=512):\n",
    "    \"\"\"Convert code to sequences for LSTM processing\"\"\"\n",
    "    \n",
    "    print(\"Creating code sequences...\")\n",
    "    \n",
    "    # Combine all code for vocabulary building\n",
    "    all_code = pd.concat([train_df['code'], test_df['code']])\n",
    "    \n",
    "    # Advanced tokenization for code\n",
    "    def preprocess_code_for_sequence(code):\n",
    "        # Keep code structure but make it more uniform\n",
    "        code = re.sub(r'/\\*.*?\\*/', ' ', code, flags=re.DOTALL)  # Remove comments\n",
    "        code = re.sub(r'//.*', ' ', code)\n",
    "        code = re.sub(r'\"[^\"]*\"', ' STRING ', code)  # Replace strings\n",
    "        code = re.sub(r\"'[^']*'\", ' CHAR ', code)   # Replace chars\n",
    "        code = re.sub(r'\\b\\d+\\b', ' NUM ', code)    # Replace numbers\n",
    "        code = re.sub(r'\\s+', ' ', code)            # Normalize whitespace\n",
    "        return code.lower().strip()\n",
    "    \n",
    "    # Preprocess all code\n",
    "    processed_code = all_code.apply(preprocess_code_for_sequence)\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=max_features,\n",
    "        oov_token='<OOV>',\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(processed_code)\n",
    "    \n",
    "    # Convert to sequences\n",
    "    train_sequences = tokenizer.texts_to_sequences(\n",
    "        train_df['code'].apply(preprocess_code_for_sequence)\n",
    "    )\n",
    "    test_sequences = tokenizer.texts_to_sequences(\n",
    "        test_df['code'].apply(preprocess_code_for_sequence)\n",
    "    )\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train_seq = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    X_test_seq = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
    "    print(f\"Sequence shape: {X_train_seq.shape}\")\n",
    "    \n",
    "    return X_train_seq, X_test_seq, tokenizer\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq, X_test_seq, tokenizer = create_code_sequences(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbd8bf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING HYBRID DATA ===\n",
      "Step 1: Creating sequences for BiLSTM...\n",
      "Creating code sequences...\n",
      "Vocabulary size: 39678\n",
      "Sequence shape: (20000, 256)\n",
      "Step 2: Creating enhanced features...\n",
      "Step 1: Applying enhanced preprocessing...\n",
      "Processing 20000 code samples...\n",
      "Step 1: Cleaning code...\n",
      "Step 2: Normalizing code...\n",
      "Step 3: Extracting enhanced security features...\n",
      "  Processing sample 0/20000\n",
      "  Processing sample 5000/20000\n",
      "  Processing sample 10000/20000\n",
      "  Processing sample 15000/20000\n",
      "Preprocessing complete!\n",
      "Enhanced features created: 54 features\n",
      "Final shape: (20000, 56)\n",
      "Processing 7000 code samples...\n",
      "Step 1: Cleaning code...\n",
      "Step 2: Normalizing code...\n",
      "Step 3: Extracting enhanced security features...\n",
      "  Processing sample 0/7000\n",
      "  Processing sample 5000/7000\n",
      "Preprocessing complete!\n",
      "Enhanced features created: 54 features\n",
      "Final shape: (7000, 55)\n",
      "Step 2: Creating TF-IDF features...\n",
      "Step 3: Using enhanced security features...\n",
      "Total enhanced features: 54\n",
      "Feature categories:\n",
      "  - Dangerous function detection: 18\n",
      "  - Vulnerability patterns: 7\n",
      "  - Code complexity: 2\n",
      "  - Security patterns: 3\n",
      "Final shapes - Train: (20000, 4054), Test: (7000, 4054)\n",
      "TF-IDF features: 4000\n",
      "Enhanced security features: 54\n",
      "Total features: 4054\n",
      "Sequence data shape: (20000, 256)\n",
      "Enhanced features shape: (20000, 4054)\n",
      "Total feature combination: sequences + 4054 engineered features\n"
     ]
    }
   ],
   "source": [
    "def prepare_hybrid_data(train_df, test_df, max_features=8000, max_len=256):\n",
    "    \"\"\"Prepare both sequence data AND enhanced features\"\"\"\n",
    "    \n",
    "    print(\"=== PREPARING HYBRID DATA ===\")\n",
    "    \n",
    "    # 1. Create sequences for BiLSTM (same as before)\n",
    "    print(\"Step 1: Creating sequences for BiLSTM...\")\n",
    "    X_train_seq, X_test_seq, tokenizer = create_code_sequences(\n",
    "        train_df, test_df, max_features, max_len\n",
    "    )\n",
    "    \n",
    "    # 2. Create enhanced features (use your existing pipeline!)\n",
    "    print(\"Step 2: Creating enhanced features...\")\n",
    "    X_train_features, X_test_features, y_train = create_pipeline_features(\n",
    "        train_df, test_df\n",
    "    )\n",
    "    \n",
    "    print(f\"Sequence data shape: {X_train_seq.shape}\")\n",
    "    print(f\"Enhanced features shape: {X_train_features.shape}\")\n",
    "    print(f\"Total feature combination: sequences + {X_train_features.shape[1]} engineered features\")\n",
    "    \n",
    "    return X_train_seq, X_test_seq, X_train_features, X_test_features, y_train, tokenizer\n",
    "\n",
    "# Prepare the hybrid data\n",
    "X_train_seq, X_test_seq, X_train_features, X_test_features, y_train, tokenizer = prepare_hybrid_data(\n",
    "    train_df, test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb2c65",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e968bc5-df0c-434b-b8fa-abb1293456e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_bilstm_features_model(vocab_size, max_len, num_features,\n",
    "                                       tokenizer=None, \n",
    "                                       embedding_dim=128, lstm_units=128):\n",
    "    \"\"\"\n",
    "    Hybrid model: BiLSTM for sequences + Enhanced features + FCNN\n",
    "    Best of both worlds!\n",
    "    \"\"\"\n",
    "    if tokenizer is not None:\n",
    "        codebert_embeddings = get_codebert_token_embeddings(tokenizer, vocab_size)\n",
    "        use_codebert = True\n",
    "    else:\n",
    "        codebert_embeddings = None\n",
    "        use_codebert = False\n",
    "        embedding_dim = 128  \n",
    "\n",
    "    sequence_input = layers.Input(shape=(max_len,), name='sequence_input')\n",
    "    \n",
    "    features_input = layers.Input(shape=(num_features,), name='enhanced_features')\n",
    "    \n",
    "    # ============ BiLSTM BRANCH (for sequential patterns) ============\n",
    "    print(\"Building BiLSTM branch for sequential code patterns...\")\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        # Embedding\n",
    "        if use_codebert:\n",
    "            # NEW: CodeBERT embeddings\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=768,  # CodeBERT dimension\n",
    "                weights=[codebert_embeddings],  # Pre-trained weights\n",
    "                trainable=False,  # Freeze CodeBERT (or True to fine-tune)\n",
    "                input_length=max_len,\n",
    "                mask_zero=True\n",
    "            )(sequence_input)\n",
    "        else:\n",
    "            # OLD: Your original embedding (as fallback)\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                input_length=max_len,\n",
    "                mask_zero=True\n",
    "            )(sequence_input)\n",
    "            \n",
    "        embedding = layers.Dropout(0.2)(embedding)\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        lstm1 = layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units, return_sequences=True, dropout=0.3)\n",
    "        )(embedding)\n",
    "        \n",
    "        lstm2 = layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units//2, return_sequences=True, dropout=0.3)\n",
    "        )(lstm1)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = layers.MultiHeadAttention(\n",
    "            num_heads=4, key_dim=lstm_units//2, dropout=0.35\n",
    "        )(lstm2, lstm2)\n",
    "        attention = layers.Add()([lstm2, attention])\n",
    "        \n",
    "        # Global pooling to get fixed-size representation\n",
    "        lstm_max_pool = layers.GlobalMaxPooling1D()(attention)\n",
    "        lstm_avg_pool = layers.GlobalAveragePooling1D()(attention)\n",
    "        lstm_output = layers.Concatenate()([lstm_max_pool, lstm_avg_pool])\n",
    "        \n",
    "        # BiLSTM feature processing\n",
    "        lstm_features = layers.Dense(512, activation='relu')(lstm_output)\n",
    "        lstm_features = layers.BatchNormalization()(lstm_features)\n",
    "        lstm_features = layers.Dropout(0.4)(lstm_features)\n",
    "        \n",
    "        lstm_features = layers.Dense(256, activation='relu')(lstm_features)\n",
    "        lstm_features = layers.BatchNormalization()(lstm_features)\n",
    "        lstm_features = layers.Dropout(0.4)(lstm_features)\n",
    "        \n",
    "        # ============ ENHANCED FEATURES BRANCH ============\n",
    "        # print(\"Building enhanced features branch...\")\n",
    "        \n",
    "        # Your enhanced feature processing (vulnerability patterns, complexity, etc.)\n",
    "        enhanced_features = layers.Dense(512, activation='relu')(features_input)\n",
    "        enhanced_features = layers.BatchNormalization()(enhanced_features)\n",
    "        enhanced_features = layers.Dropout(0.5)(enhanced_features)\n",
    "        \n",
    "        enhanced_features = layers.Dense(256, activation='relu')(enhanced_features)\n",
    "        enhanced_features = layers.BatchNormalization()(enhanced_features)\n",
    "        enhanced_features = layers.Dropout(0.4)(enhanced_features)\n",
    "        \n",
    "        enhanced_features = layers.Dense(128, activation='relu')(enhanced_features)\n",
    "        enhanced_features = layers.BatchNormalization()(enhanced_features)\n",
    "        enhanced_features = layers.Dropout(0.4)(enhanced_features)\n",
    "        \n",
    "        # ============ FUSION LAYER ============\n",
    "        # print(\"Combining BiLSTM and enhanced features...\")\n",
    "        \n",
    "        # Combine both branches\n",
    "        combined_features = layers.Concatenate(name='feature_fusion')([\n",
    "            lstm_features,      # Sequential patterns from BiLSTM\n",
    "            enhanced_features   # Engineered vulnerability features\n",
    "        ])\n",
    "        \n",
    "        # ============ FINAL FCNN (for classification) ============\n",
    "        # print(\"Building final classification network...\")\n",
    "        \n",
    "        # Deep FCNN for final classification\n",
    "        x = layers.Dense(1024, activation='relu')(combined_features)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.6)(x)\n",
    "        \n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        \n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.Dropout(0.35)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = layers.Dense(1, activation='sigmoid', name='vulnerability_prediction')(x)\n",
    "        \n",
    "        # Create model\n",
    "        model = keras.Model(\n",
    "            inputs=[sequence_input, features_input],\n",
    "            outputs=output,\n",
    "            name='Hybrid_BiLSTM_Features_Model'\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66fd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_model(X_train_seq, X_train_features, y_train, \n",
    "                      validation_split=0.2):\n",
    "    \"\"\"Train the hybrid BiLSTM + Enhanced Features model\"\"\"\n",
    "    \n",
    "    print(\"=== TRAINING HYBRID MODEL ===\")\n",
    "    \n",
    "    # Create model\n",
    "    vocab_size = min(8000, len(tokenizer.word_index) + 1)\n",
    "    model = create_hybrid_bilstm_features_model(\n",
    "        vocab_size=vocab_size,\n",
    "        max_len=X_train_seq.shape[1],\n",
    "        num_features=X_train_features.shape[1],\n",
    "        embedding_dim=128,\n",
    "        lstm_units=128,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Enhanced callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc', patience=10, restore_best_weights=True, mode='max'\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc', factor=0.5, patience=7, min_lr=1e-7, mode='max'\n",
    "        ),\n",
    "        # keras.callbacks.ModelCheckpoint(\n",
    "        #     'best_hybrid_model.h5', monitor='val_auc', save_best_only=True, mode='max'\n",
    "        # )\n",
    "    ]\n",
    "    \n",
    "    print(\"Training hybrid model (BiLSTM + Enhanced Features)...\")\n",
    "    \n",
    "    # Train with BOTH inputs\n",
    "    history = model.fit(\n",
    "        [X_train_seq, X_train_features],  # Both sequence and feature inputs\n",
    "        y_train,\n",
    "        validation_split=validation_split,\n",
    "        epochs=100,\n",
    "        batch_size=128,  # Good balance for both LSTM and dense layers\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train the hybrid model\n",
    "hybrid_model, history = train_hybrid_model(X_train_seq, X_train_features, y_train)\n",
    "\n",
    "# Model summary\n",
    "print(\"\\n=== HYBRID MODEL ARCHITECTURE ===\")\n",
    "hybrid_model.summary()\n",
    "\n",
    "# Evaluate\n",
    "val_split_idx = int(len(y_train) * 0.8)\n",
    "X_val_seq = X_train_seq[val_split_idx:]\n",
    "X_val_features = X_train_features[val_split_idx:]\n",
    "y_val = y_train[val_split_idx:]\n",
    "\n",
    "# ont rain\n",
    "train_predictions = hybrid_model.predict([X_train_seq, X_train_features])\n",
    "train_auc = roc_auc_score(y_train, train_predictions)\n",
    "print(f\"\\nHybrid Model Train AUC: {train_auc:.4f}\")\n",
    "\n",
    "val_predictions = hybrid_model.predict([X_val_seq, X_val_features])\n",
    "val_auc = roc_auc_score(y_val, val_predictions)\n",
    "print(f\"\\nHybrid Model Validation AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions on test set\n",
    "\n",
    "test_predictions = hybrid_model.predict([X_test_seq, X_test_features])\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Prediction range: {test_predictions.min():.4f} - {test_predictions.max():.4f}\")\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "test_predictions_binary = (test_predictions > 0.35).astype(int)\n",
    "\n",
    "print(f\"\\nAfter converting to binary:\")\n",
    "print(f\"Binary predictions shape: {test_predictions_binary.shape}\")\n",
    "print(f\"Binary prediction values: {np.unique(test_predictions_binary, return_counts=True)}\")\n",
    "print(f\"Sample predictions:\")\n",
    "print(f\"  Probabilities: {test_predictions[:10].flatten()}\")\n",
    "print(f\"  Binary:        {test_predictions_binary[:10].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_df = sample_submission.copy()\n",
    "\n",
    "# Option 1: Use binary predictions (0 or 1)\n",
    "submission_df['Label'] = test_predictions_binary.flatten()\n",
    "\n",
    "# Option 2: Use probability predictions (often better for competitions)\n",
    "# submission_df['Label'] = test_predictions.flatten()\n",
    "\n",
    "print(\"Submission file created!\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "print(f\"Label distribution:\")\n",
    "print(submission_df['Label'].value_counts())\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Save submission\n",
    "test_time = test_time + 1\n",
    "submission_df.to_csv(f'data/hybrid_submission_0{test_time}.csv', index=False)\n",
    "print(f\"\\nSubmission saved as 'hybrid_submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cebfe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Version for Hybrid BiLSTM Model\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "def create_hybrid_bilstm_gridsearch_model(vocab_size, max_len, num_features,\n",
    "                                         tokenizer=None, \n",
    "                                         embedding_dim=128, \n",
    "                                         lstm_units=128,\n",
    "                                         dropout_rate=0.3,\n",
    "                                         dense_units=[512, 256]):\n",
    "    \"\"\"\n",
    "    Parameterized version of the hybrid model for grid search (no CodeBERT)\n",
    "    \"\"\"\n",
    "    # Always use standard embeddings for grid search (faster and more stable)\n",
    "    codebert_embeddings = None\n",
    "    use_codebert = False\n",
    "    embedding_dim = 128\n",
    "    print(\"Using standard embeddings (CodeBERT disabled for grid search)\")\n",
    "\n",
    "    # Input layers\n",
    "    sequence_input = layers.Input(shape=(max_len,), name='sequence_input')\n",
    "    features_input = layers.Input(shape=(num_features,), name='enhanced_features')\n",
    "    \n",
    "    # ============ BiLSTM BRANCH ============\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Embedding\n",
    "        if use_codebert:\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=768,\n",
    "                weights=[codebert_embeddings],\n",
    "                trainable=False,\n",
    "                input_length=max_len,\n",
    "                mask_zero=True\n",
    "            )(sequence_input)\n",
    "        else:\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                input_length=max_len,\n",
    "                mask_zero=True\n",
    "            )(sequence_input)\n",
    "            \n",
    "        embedding = layers.Dropout(dropout_rate * 0.7)(embedding)  # Slightly less dropout on embedding\n",
    "        \n",
    "        # BiLSTM layers with parameterized units\n",
    "        lstm1 = layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units, return_sequences=True, dropout=dropout_rate)\n",
    "        )(embedding)\n",
    "        \n",
    "        lstm2 = layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units//2, return_sequences=True, dropout=dropout_rate)\n",
    "        )(lstm1)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention = layers.MultiHeadAttention(\n",
    "            num_heads=4, key_dim=lstm_units//2, dropout=dropout_rate + 0.05\n",
    "        )(lstm2, lstm2)\n",
    "        attention = layers.Add()([lstm2, attention])\n",
    "        \n",
    "        # Global pooling\n",
    "        lstm_max_pool = layers.GlobalMaxPooling1D()(attention)\n",
    "        lstm_avg_pool = layers.GlobalAveragePooling1D()(attention)\n",
    "        lstm_output = layers.Concatenate()([lstm_max_pool, lstm_avg_pool])\n",
    "        \n",
    "        # BiLSTM feature processing with parameterized dense units\n",
    "        lstm_features = layers.Dense(dense_units[0], activation='relu')(lstm_output)\n",
    "        lstm_features = layers.BatchNormalization()(lstm_features)\n",
    "        lstm_features = layers.Dropout(dropout_rate + 0.1)(lstm_features)\n",
    "        \n",
    "        lstm_features = layers.Dense(dense_units[1], activation='relu')(lstm_features)\n",
    "        lstm_features = layers.BatchNormalization()(lstm_features)\n",
    "        lstm_features = layers.Dropout(dropout_rate + 0.1)(lstm_features)\n",
    "        \n",
    "        # ============ ENHANCED FEATURES BRANCH ============\n",
    "        enhanced_features = layers.Dense(dense_units[0], activation='relu')(features_input)\n",
    "        enhanced_features = layers.BatchNormalization()(enhanced_features)\n",
    "        enhanced_features = layers.Dropout(dropout_rate + 0.2)(enhanced_features)\n",
    "        \n",
    "        enhanced_features = layers.Dense(dense_units[1], activation='relu')(enhanced_features)\n",
    "        enhanced_features = layers.BatchNormalization()(enhanced_features)\n",
    "        enhanced_features = layers.Dropout(dropout_rate + 0.1)(enhanced_features)\n",
    "        \n",
    "        enhanced_features = layers.Dense(dense_units[1]//2, activation='relu')(enhanced_features)\n",
    "        enhanced_features = layers.BatchNormalization()(enhanced_features)\n",
    "        enhanced_features = layers.Dropout(dropout_rate + 0.1)(enhanced_features)\n",
    "        \n",
    "        # ============ FUSION LAYER ============\n",
    "        combined_features = layers.Concatenate(name='feature_fusion')([\n",
    "            lstm_features,      \n",
    "            enhanced_features   \n",
    "        ])\n",
    "        \n",
    "        # ============ FINAL FCNN ============\n",
    "        # Parameterized final dense layers\n",
    "        x = layers.Dense(dense_units[0] * 2, activation='relu')(combined_features)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate + 0.3)(x)\n",
    "        \n",
    "        x = layers.Dense(dense_units[0], activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate + 0.2)(x)\n",
    "        \n",
    "        x = layers.Dense(dense_units[1], activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate + 0.1)(x)\n",
    "        \n",
    "        x = layers.Dense(dense_units[1]//4, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate + 0.1)(x)\n",
    "        \n",
    "        x = layers.Dense(dense_units[1]//8, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate + 0.05)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = layers.Dense(1, activation='sigmoid', name='vulnerability_prediction')(x)\n",
    "        \n",
    "        # Create model\n",
    "        model = keras.Model(\n",
    "            inputs=[sequence_input, features_input],\n",
    "            outputs=output,\n",
    "            name='Hybrid_BiLSTM_GridSearch_Model'\n",
    "        )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e97ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hybrid_model(X_train_seq, X_train_features, y_train, \n",
    "                            X_val_seq, X_val_features, y_val,\n",
    "                            tokenizer, top_k=10):\n",
    "    \"\"\"\n",
    "    Comprehensive grid search for hybrid BiLSTM model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.001, 0.0005, 0.002, 0.0015],\n",
    "        'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "        'lstm_units': [64, 96, 128, 160],\n",
    "        'dense_units': [\n",
    "            [256, 128],   # Smaller network\n",
    "            [384, 192],   # Medium network  \n",
    "            [512, 256],   # Original size\n",
    "            [640, 320],   # Larger network\n",
    "            [768, 384]    # Even larger\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate all combinations\n",
    "    param_combinations = list(itertools.product(\n",
    "        param_grid['learning_rate'],\n",
    "        param_grid['dropout_rate'], \n",
    "        param_grid['lstm_units'],\n",
    "        param_grid['dense_units']\n",
    "    ))\n",
    "    \n",
    "    print(f\"Total parameter combinations to test: {len(param_combinations)}\")\n",
    "    print(\"This will take a while - grab some coffee! \")\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Model parameters (fixed)\n",
    "    vocab_size = min(8000, len(tokenizer.word_index) + 1)\n",
    "    max_len = X_train_seq.shape[1]\n",
    "    num_features = X_train_features.shape[1]\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i, (lr, dropout, lstm_units, dense_units) in enumerate(param_combinations):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing combination {i+1}/{len(param_combinations)}\")\n",
    "        print(f\"LR: {lr}, Dropout: {dropout}, LSTM Units: {lstm_units}, Dense: {dense_units}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Clear any previous models from memory\n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            print(\"Creating model without CodeBERT embeddings...\")\n",
    "            # Create model with current parameters (without CodeBERT)\n",
    "            model = create_hybrid_bilstm_gridsearch_model(\n",
    "                vocab_size=vocab_size,\n",
    "                max_len=max_len,\n",
    "                num_features=num_features,\n",
    "                tokenizer=None,  # Disable CodeBERT embeddings\n",
    "                lstm_units=lstm_units,\n",
    "                dropout_rate=dropout,\n",
    "                dense_units=dense_units\n",
    "            )\n",
    "            \n",
    "            # Compile model\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.AdamW(learning_rate=lr, weight_decay=0.01),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=[keras.metrics.AUC(name='auc')]\n",
    "            )\n",
    "            \n",
    "            # Callbacks for this specific run\n",
    "            callbacks = [\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_auc', patience=8, restore_best_weights=True, mode='max'\n",
    "                ),\n",
    "                keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_auc', factor=0.7, patience=5, min_lr=1e-7, mode='max'\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Train model (fewer epochs for grid search)\n",
    "            history = model.fit(\n",
    "                [X_train_seq, X_train_features],\n",
    "                y_train,\n",
    "                validation_data=([X_val_seq, X_val_features], y_val),\n",
    "                epochs=30,  # Reduced for grid search\n",
    "                batch_size=128,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0  # Silent training\n",
    "            )\n",
    "            \n",
    "            # Get best validation AUC\n",
    "            best_val_auc = max(history.history['val_auc'])\n",
    "            best_train_auc = max(history.history['auc'])\n",
    "            final_val_loss = min(history.history['val_loss'])\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'combination_id': i,\n",
    "                'learning_rate': lr,\n",
    "                'dropout_rate': dropout,\n",
    "                'lstm_units': lstm_units,\n",
    "                'dense_units': dense_units,\n",
    "                'best_val_auc': best_val_auc,\n",
    "                'best_train_auc': best_train_auc,\n",
    "                'final_val_loss': final_val_loss,\n",
    "                'overfitting_score': best_train_auc - best_val_auc,  # Lower is better\n",
    "                'epochs_trained': len(history.history['val_auc']),\n",
    "                'model': model,  # Save the actual model\n",
    "                'history': history.history\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\" Validation AUC: {best_val_auc:.4f}\")\n",
    "            print(f\" Train AUC: {best_train_auc:.4f}\")\n",
    "            print(f\" Overfitting: {result['overfitting_score']:.4f}\")\n",
    "            print(f\"  Epochs: {result['epochs_trained']}\")\n",
    "            \n",
    "            # Show current top performers\n",
    "            if len(results) >= 3:\n",
    "                current_top = sorted(results, key=lambda x: x['best_val_auc'], reverse=True)[:3]\n",
    "                print(f\"\\n Current Top 3:\")\n",
    "                for j, r in enumerate(current_top):\n",
    "                    print(f\"  {j+1}. AUC: {r['best_val_auc']:.4f} \"\n",
    "                          f\"(LR: {r['learning_rate']}, Drop: {r['dropout_rate']}, \"\n",
    "                          f\"LSTM: {r['lstm_units']}, Dense: {r['dense_units']})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error with combination {i+1}: {str(e)}\")\n",
    "            # Continue with next combination\n",
    "            continue\n",
    "            \n",
    "        # Memory cleanup\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Sort results by validation AUC\n",
    "    results.sort(key=lambda x: x['best_val_auc'], reverse=True)\n",
    "    \n",
    "    # Keep only top K results\n",
    "    top_results = results[:top_k]\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" GRID SEARCH COMPLETED!\")\n",
    "    print(f\" Total time: {total_time}\")\n",
    "    print(f\" Combinations tested: {len(results)}\")\n",
    "    print(f\" Top {len(top_results)} models saved\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return top_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_grid_search_results(top_results):\n",
    "    \"\"\"Display comprehensive results from grid search\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\" TOP {len(top_results)} HYBRID BiLSTM MODELS\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Create results DataFrame for easy viewing\n",
    "    results_data = []\n",
    "    for i, result in enumerate(top_results):\n",
    "        results_data.append({\n",
    "            'Rank': i + 1,\n",
    "            'Val_AUC': result['best_val_auc'],\n",
    "            'Train_AUC': result['best_train_auc'],\n",
    "            'Overfitting': result['overfitting_score'],\n",
    "            'Learning_Rate': result['learning_rate'],\n",
    "            'Dropout': result['dropout_rate'],\n",
    "            'LSTM_Units': result['lstm_units'],\n",
    "            'Dense_Units': str(result['dense_units']),\n",
    "            'Epochs': result['epochs_trained'],\n",
    "            'Val_Loss': result['final_val_loss']\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\" HYPERPARAMETER ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Best learning rates\n",
    "    lr_analysis = {}\n",
    "    for result in top_results:\n",
    "        lr = result['learning_rate']\n",
    "        if lr not in lr_analysis:\n",
    "            lr_analysis[lr] = []\n",
    "        lr_analysis[lr].append(result['best_val_auc'])\n",
    "    \n",
    "    print(\" Learning Rate Performance:\")\n",
    "    for lr in sorted(lr_analysis.keys()):\n",
    "        aucs = lr_analysis[lr]\n",
    "        print(f\"  LR {lr}: Avg AUC = {np.mean(aucs):.4f} (appeared {len(aucs)} times in top {len(top_results)})\")\n",
    "    \n",
    "    # Best dropout rates\n",
    "    dropout_analysis = {}\n",
    "    for result in top_results:\n",
    "        dropout = result['dropout_rate']\n",
    "        if dropout not in dropout_analysis:\n",
    "            dropout_analysis[dropout] = []\n",
    "        dropout_analysis[dropout].append(result['best_val_auc'])\n",
    "    \n",
    "    print(\"\\n Dropout Rate Performance:\")\n",
    "    for dropout in sorted(dropout_analysis.keys()):\n",
    "        aucs = dropout_analysis[dropout]\n",
    "        print(f\"  Dropout {dropout}: Avg AUC = {np.mean(aucs):.4f} (appeared {len(aucs)} times in top {len(top_results)})\")\n",
    "    \n",
    "    # Best LSTM units\n",
    "    lstm_analysis = {}\n",
    "    for result in top_results:\n",
    "        lstm_units = result['lstm_units']\n",
    "        if lstm_units not in lstm_analysis:\n",
    "            lstm_analysis[lstm_units] = []\n",
    "        lstm_analysis[lstm_units].append(result['best_val_auc'])\n",
    "    \n",
    "    print(\"\\n LSTM Units Performance:\")\n",
    "    for units in sorted(lstm_analysis.keys()):\n",
    "        aucs = lstm_analysis[units]\n",
    "        print(f\"  LSTM {units}: Avg AUC = {np.mean(aucs):.4f} (appeared {len(aucs)} times in top {len(top_results)})\")\n",
    "    \n",
    "    # Best dense architectures\n",
    "    dense_analysis = {}\n",
    "    for result in top_results:\n",
    "        dense_str = str(result['dense_units'])\n",
    "        if dense_str not in dense_analysis:\n",
    "            dense_analysis[dense_str] = []\n",
    "        dense_analysis[dense_str].append(result['best_val_auc'])\n",
    "    \n",
    "    print(\"\\n  Dense Architecture Performance:\")\n",
    "    for arch in dense_analysis:\n",
    "        aucs = dense_analysis[arch]\n",
    "        print(f\"  Dense {arch}: Avg AUC = {np.mean(aucs):.4f} (appeared {len(aucs)} times in top {len(top_results)})\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"  CHAMPION MODEL DETAILS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    champion = top_results[0]\n",
    "    print(f\" Best Model Configuration:\")\n",
    "    print(f\"   Validation AUC: {champion['best_val_auc']:.6f}\")\n",
    "    print(f\"   Train AUC: {champion['best_train_auc']:.6f}\")\n",
    "    print(f\"   Overfitting Score: {champion['overfitting_score']:.6f}\")\n",
    "    print(f\"   Learning Rate: {champion['learning_rate']}\")\n",
    "    print(f\"   Dropout Rate: {champion['dropout_rate']}\")\n",
    "    print(f\"   LSTM Units: {champion['lstm_units']}\")\n",
    "    print(f\"   Dense Units: {champion['dense_units']}\")\n",
    "    print(f\"   Epochs Trained: {champion['epochs_trained']}\")\n",
    "    print(f\"   Final Val Loss: {champion['final_val_loss']:.6f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def save_top_models(top_results, save_dir=\"gridsearch_models\"):\n",
    "    \"\"\"Save the top models and results\"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save models and metadata\n",
    "    saved_models = []\n",
    "    \n",
    "    for i, result in enumerate(top_results):\n",
    "        model_name = f\"rank_{i+1}_auc_{result['best_val_auc']:.4f}\"\n",
    "        model_path = os.path.join(save_dir, f\"{model_name}.h5\")\n",
    "        \n",
    "        # Save model\n",
    "        result['model'].save(model_path)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {k: v for k, v in result.items() if k != 'model'}  # Exclude model object\n",
    "        metadata_path = os.path.join(save_dir, f\"{model_name}_metadata.pkl\")\n",
    "        \n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        saved_models.append({\n",
    "            'rank': i + 1,\n",
    "            'model_path': model_path,\n",
    "            'metadata_path': metadata_path,\n",
    "            'val_auc': result['best_val_auc'],\n",
    "            'config': {\n",
    "                'learning_rate': result['learning_rate'],\n",
    "                'dropout_rate': result['dropout_rate'],\n",
    "                'lstm_units': result['lstm_units'],\n",
    "                'dense_units': result['dense_units']\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        print(f\" Saved model {i+1}/{len(top_results)}: {model_name}\")\n",
    "    \n",
    "    # Save overall results summary\n",
    "    summary_path = os.path.join(save_dir, \"gridsearch_summary.pkl\")\n",
    "    with open(summary_path, 'wb') as f:\n",
    "        pickle.dump(saved_models, f)\n",
    "    \n",
    "    print(f\"\\n All {len(top_results)} models saved to '{save_dir}/' directory\")\n",
    "    print(f\" Summary saved to: {summary_path}\")\n",
    "    \n",
    "    return saved_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b76523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN GRID SEARCH - This will take several hours!\n",
    "print(\" STARTING COMPREHENSIVE GRID SEARCH\")\n",
    "print(\"  Warning: This will test 320 parameter combinations and may take 4-8 hours!\")\n",
    "print(\" Tip: Consider running on a machine with good GPU and leaving it overnight\")\n",
    "\n",
    "# Prepare validation split (use the same split as before for consistency)\n",
    "val_split_idx = int(len(y_train) * 0.8)\n",
    "\n",
    "X_train_seq_gs = X_train_seq[:val_split_idx]\n",
    "X_train_features_gs = X_train_features[:val_split_idx]\n",
    "y_train_gs = y_train[:val_split_idx]\n",
    "\n",
    "X_val_seq_gs = X_train_seq[val_split_idx:]\n",
    "X_val_features_gs = X_train_features[val_split_idx:]\n",
    "y_val_gs = y_train[val_split_idx:]\n",
    "\n",
    "print(f\"Grid search training set: {X_train_seq_gs.shape[0]} samples\")\n",
    "print(f\"Grid search validation set: {X_val_seq_gs.shape[0]} samples\")\n",
    "\n",
    "# Run grid search\n",
    "top_models = grid_search_hybrid_model(\n",
    "    X_train_seq_gs, X_train_features_gs, y_train_gs,\n",
    "    X_val_seq_gs, X_val_features_gs, y_val_gs,\n",
    "    tokenizer, \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# Display results\n",
    "results_df = display_grid_search_results(top_models)\n",
    "\n",
    "# Save all top models\n",
    "saved_models_info = save_top_models(top_models)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\" GRID SEARCH SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\" Best validation AUC achieved: {top_models[0]['best_val_auc']:.6f}\")\n",
    "print(f\" This is a {(top_models[0]['best_val_auc'] - 0.63)*100:.2f}% improvement over baseline (0.63)\")\n",
    "print(f\" Top 10 models saved for ensemble/production use\")\n",
    "print(f\" Results DataFrame available as 'results_df'\")\n",
    "print(f\" Champion model available as 'top_models[0]['model']'\")\n",
    "\n",
    "# Quick test with the champion model\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" TESTING CHAMPION MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "champion_model = top_models[0]['model']\n",
    "\n",
    "# Test on validation set\n",
    "val_predictions = champion_model.predict([X_val_seq_gs, X_val_features_gs])\n",
    "val_auc = roc_auc_score(y_val_gs, val_predictions)\n",
    "print(f\" Champion model validation AUC: {val_auc:.6f}\")\n",
    "\n",
    "# Test on full training set\n",
    "train_predictions = champion_model.predict([X_train_seq, X_train_features])\n",
    "train_auc = roc_auc_score(y_train, train_predictions)\n",
    "print(f\" Champion model full training AUC: {train_auc:.6f}\")\n",
    "\n",
    "# Generate predictions for test set\n",
    "test_predictions_champion = champion_model.predict([X_test_seq, X_test_features])\n",
    "\n",
    "print(f\"\\n GRID SEARCH COMPLETE!\")\n",
    "print(f\"Champion model ready for submission generation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission from the champion model\n",
    "def create_champion_submission(test_predictions, threshold=0.5):\n",
    "    \"\"\"Create submission file from champion model predictions\"\"\"\n",
    "    \n",
    "    submission_df = sample_submission.copy()\n",
    "    \n",
    "    # Option 1: Binary predictions with custom threshold\n",
    "    binary_predictions = (test_predictions > threshold).astype(int)\n",
    "    submission_df['Label'] = binary_predictions.flatten()\n",
    "    \n",
    "    print(f\"Champion Model Submission Summary:\")\n",
    "    print(f\"Threshold used: {threshold}\")\n",
    "    print(f\"Prediction distribution:\")\n",
    "    print(submission_df['Label'].value_counts())\n",
    "    print(f\"Percentage of insecure predictions: {(submission_df['Label'].sum() / len(submission_df) * 100):.2f}%\")\n",
    "    \n",
    "    # Save submission\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    submission_filename = f\"champion_submission_{timestamp}.csv\"\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    \n",
    "    print(f\" Champion submission saved as: {submission_filename}\")\n",
    "    \n",
    "    return submission_df, submission_filename\n",
    "\n",
    "# Create submission with different thresholds to optimize\n",
    "print(\" Creating optimized submissions from champion model...\")\n",
    "\n",
    "thresholds_to_try = [0.3, 0.35, 0.4, 0.45, 0.5, 0.55]\n",
    "\n",
    "for threshold in thresholds_to_try:\n",
    "    submission_df, filename = create_champion_submission(test_predictions_champion, threshold)\n",
    "    print(f\"Created: {filename}\")\n",
    "\n",
    "print(f\"\\n GRID SEARCH PROJECT COMPLETE!\")\n",
    "print(f\"You now have:\")\n",
    "print(f\"   Top 10 tuned models saved\")\n",
    "print(f\"   Comprehensive hyperparameter analysis\")\n",
    "print(f\"   Champion model ready for deployment\")\n",
    "print(f\"   Multiple submission files with different thresholds\")\n",
    "print(f\"   Complete grid search results for future reference\")\n",
    "\n",
    "# Save a final summary file\n",
    "final_summary = {\n",
    "    'grid_search_completed': datetime.now().isoformat(),\n",
    "    'total_combinations_tested': len([x for x in dir() if 'results' in str(x)]),\n",
    "    'best_val_auc': top_models[0]['best_val_auc'] if 'top_models' in locals() else 'Grid search not run yet',\n",
    "    'champion_config': top_models[0] if 'top_models' in locals() else 'Grid search not run yet',\n",
    "    'submissions_created': [f for f in locals() if 'champion_submission' in str(f)]\n",
    "}\n",
    "\n",
    "with open('gridsearch_final_summary.txt', 'w') as f:\n",
    "    f.write(\"HYBRID BiLSTM GRID SEARCH - FINAL SUMMARY\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    for key, value in final_summary.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "print(f\"\\n Final summary saved to: gridsearch_final_summary.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ed1878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TRAINING TOP 3 GRID SEARCH CONFIGURATIONS\n",
      "======================================================================\n",
      "\n",
      " Training Champion Model (top_1)\n",
      "   LR: 0.001, Dropout: 0.3\n",
      "   LSTM Units: 96, Dense: [768, 384]\n",
      "--------------------------------------------------\n",
      "Using standard embeddings (CodeBERT disabled for grid search)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754414040.539428  122191 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754414050.975981  122417 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - auc: 0.5007 - loss: 0.8576"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 123ms/step - auc: 0.5050 - loss: 0.7843 - val_auc: 0.5313 - val_loss: 0.6894 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 111ms/step - auc: 0.5143 - loss: 0.7035 - val_auc: 0.4760 - val_loss: 0.6919 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - auc: 0.5062 - loss: 0.6955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.5128 - loss: 0.6942 - val_auc: 0.5448 - val_loss: 0.6890 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - auc: 0.5175 - loss: 0.6919"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 118ms/step - auc: 0.5297 - loss: 0.6902 - val_auc: 0.5515 - val_loss: 0.6896 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - auc: 0.5411 - loss: 0.6873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - auc: 0.5493 - loss: 0.6850 - val_auc: 0.5888 - val_loss: 0.6751 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - auc: 0.5813 - loss: 0.6746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 118ms/step - auc: 0.5901 - loss: 0.6731 - val_auc: 0.6096 - val_loss: 0.6619 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - auc: 0.6391 - loss: 0.6552"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - auc: 0.6319 - loss: 0.6571 - val_auc: 0.6211 - val_loss: 0.6718 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - auc: 0.6684 - loss: 0.6399"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - auc: 0.6684 - loss: 0.6395 - val_auc: 0.6319 - val_loss: 0.6563 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.6962 - loss: 0.6234 - val_auc: 0.6277 - val_loss: 0.6578 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.7155 - loss: 0.6100 - val_auc: 0.6155 - val_loss: 0.7376 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - auc: 0.7527 - loss: 0.5801"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - auc: 0.7452 - loss: 0.5859 - val_auc: 0.6433 - val_loss: 0.7408 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - auc: 0.7838 - loss: 0.5529"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 118ms/step - auc: 0.7754 - loss: 0.5610 - val_auc: 0.6569 - val_loss: 0.6977 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - auc: 0.8211 - loss: 0.5142"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - auc: 0.8099 - loss: 0.5266 - val_auc: 0.6675 - val_loss: 0.7473 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - auc: 0.8360 - loss: 0.4919"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.8272 - loss: 0.5048 - val_auc: 0.6716 - val_loss: 0.6599 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.8468 - loss: 0.4813 - val_auc: 0.6634 - val_loss: 0.7486 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.8670 - loss: 0.4510 - val_auc: 0.6580 - val_loss: 1.1013 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - auc: 0.8777 - loss: 0.4352 - val_auc: 0.6646 - val_loss: 0.7792 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.8958 - loss: 0.4052 - val_auc: 0.6616 - val_loss: 0.7631 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 111ms/step - auc: 0.9096 - loss: 0.3793 - val_auc: 0.6595 - val_loss: 0.9688 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.9174 - loss: 0.3621 - val_auc: 0.6578 - val_loss: 1.0025 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - auc: 0.9300 - loss: 0.3355 - val_auc: 0.6530 - val_loss: 0.9332 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.9414 - loss: 0.3083 - val_auc: 0.6526 - val_loss: 0.9959 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.9517 - loss: 0.2802 - val_auc: 0.6404 - val_loss: 1.2577 - learning_rate: 7.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - auc: 0.9618 - loss: 0.2483 - val_auc: 0.6470 - val_loss: 1.3271 - learning_rate: 7.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.9648 - loss: 0.2399 - val_auc: 0.6445 - val_loss: 1.4068 - learning_rate: 7.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.9680 - loss: 0.2285 - val_auc: 0.6428 - val_loss: 1.2696 - learning_rate: 7.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.9715 - loss: 0.2168 - val_auc: 0.6338 - val_loss: 1.6591 - learning_rate: 7.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - auc: 0.9730 - loss: 0.2094 - val_auc: 0.6399 - val_loss: 1.5766 - learning_rate: 7.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 123ms/step - auc: 0.9771 - loss: 0.1934 - val_auc: 0.6364 - val_loss: 1.6358 - learning_rate: 7.0000e-04\n",
      " Champion Model - Best Validation AUC: 0.671613\n",
      " Expected AUC: 0.6977, Achieved: 0.6716\n",
      " Generating predictions for Champion model...\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step\n",
      "   Prediction range: 0.0073 - 0.9998\n",
      "\n",
      " Training Runner-up Model (top_2)\n",
      "   LR: 0.001, Dropout: 0.2\n",
      "   LSTM Units: 128, Dense: [640, 320]\n",
      "--------------------------------------------------\n",
      "Using standard embeddings (CodeBERT disabled for grid search)\n",
      "Epoch 1/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - auc: 0.5229 - loss: 0.7611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 124ms/step - auc: 0.5214 - loss: 0.7312 - val_auc: 0.4640 - val_loss: 0.7054 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - auc: 0.5255 - loss: 0.6966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - auc: 0.5359 - loss: 0.6914 - val_auc: 0.5777 - val_loss: 0.6794 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - auc: 0.5677 - loss: 0.6788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - auc: 0.5631 - loss: 0.6782 - val_auc: 0.6033 - val_loss: 0.6698 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - auc: 0.6060 - loss: 0.6627"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - auc: 0.6129 - loss: 0.6626 - val_auc: 0.6119 - val_loss: 0.6644 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - auc: 0.6514 - loss: 0.6466 - val_auc: 0.6068 - val_loss: 0.6613 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - auc: 0.6836 - loss: 0.6263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.6819 - loss: 0.6303 - val_auc: 0.6269 - val_loss: 0.6676 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.7098 - loss: 0.6110 - val_auc: 0.6258 - val_loss: 0.6675 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - auc: 0.7358 - loss: 0.5920"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 124ms/step - auc: 0.7305 - loss: 0.5961 - val_auc: 0.6340 - val_loss: 0.6598 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.7503 - loss: 0.5798 - val_auc: 0.6333 - val_loss: 0.6667 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - auc: 0.7746 - loss: 0.5570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - auc: 0.7710 - loss: 0.5603 - val_auc: 0.6413 - val_loss: 0.6894 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.7871 - loss: 0.5443 - val_auc: 0.6303 - val_loss: 0.6909 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - auc: 0.7975 - loss: 0.5337 - val_auc: 0.6348 - val_loss: 0.6888 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 114ms/step - auc: 0.8177 - loss: 0.5105 - val_auc: 0.6257 - val_loss: 0.6849 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.8337 - loss: 0.4892 - val_auc: 0.6329 - val_loss: 0.7406 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.8427 - loss: 0.4767 - val_auc: 0.6290 - val_loss: 0.7310 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 120ms/step - auc: 0.8537 - loss: 0.4623 - val_auc: 0.6281 - val_loss: 0.7398 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 118ms/step - auc: 0.8614 - loss: 0.4514 - val_auc: 0.6195 - val_loss: 0.7534 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.8725 - loss: 0.4340 - val_auc: 0.6268 - val_loss: 0.9150 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - auc: 0.8883 - loss: 0.4082 - val_auc: 0.6207 - val_loss: 0.8820 - learning_rate: 7.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - auc: 0.8996 - loss: 0.3885 - val_auc: 0.6294 - val_loss: 0.9008 - learning_rate: 7.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - auc: 0.9016 - loss: 0.3856 - val_auc: 0.6349 - val_loss: 0.8746 - learning_rate: 7.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - auc: 0.9127 - loss: 0.3655 - val_auc: 0.6313 - val_loss: 0.8443 - learning_rate: 7.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - auc: 0.9161 - loss: 0.3590 - val_auc: 0.6276 - val_loss: 0.9999 - learning_rate: 7.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.9220 - loss: 0.3456 - val_auc: 0.6281 - val_loss: 1.0550 - learning_rate: 7.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - auc: 0.9280 - loss: 0.3340 - val_auc: 0.6235 - val_loss: 1.0295 - learning_rate: 7.0000e-04\n",
      " Runner-up Model - Best Validation AUC: 0.641331\n",
      " Expected AUC: 0.6969, Achieved: 0.6413\n",
      " Generating predictions for Runner-up model...\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step\n",
      "   Prediction range: 0.0060 - 1.0000\n",
      "\n",
      " Training Third Model (top_3)\n",
      "   LR: 0.001, Dropout: 0.2\n",
      "   LSTM Units: 96, Dense: [384, 192]\n",
      "--------------------------------------------------\n",
      "Using standard embeddings (CodeBERT disabled for grid search)\n",
      "Epoch 1/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - auc: 0.5030 - loss: 0.7737"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 120ms/step - auc: 0.5105 - loss: 0.7362 - val_auc: 0.5362 - val_loss: 0.6899 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - auc: 0.5188 - loss: 0.6988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.5179 - loss: 0.6975 - val_auc: 0.5579 - val_loss: 0.6881 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - auc: 0.5292 - loss: 0.6888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.5383 - loss: 0.6860 - val_auc: 0.5809 - val_loss: 0.6742 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - auc: 0.5738 - loss: 0.6754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.5802 - loss: 0.6742 - val_auc: 0.6050 - val_loss: 0.6672 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - auc: 0.6133 - loss: 0.6631"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.6262 - loss: 0.6574 - val_auc: 0.6441 - val_loss: 0.6613 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - auc: 0.6891 - loss: 0.6249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.6884 - loss: 0.6273 - val_auc: 0.6590 - val_loss: 0.6423 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - auc: 0.7373 - loss: 0.5921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.7371 - loss: 0.5926 - val_auc: 0.6819 - val_loss: 0.6291 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - auc: 0.7791 - loss: 0.5570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - auc: 0.7766 - loss: 0.5602 - val_auc: 0.6876 - val_loss: 0.6614 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.8005 - loss: 0.5357 - val_auc: 0.6802 - val_loss: 0.6493 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.8231 - loss: 0.5071 - val_auc: 0.6808 - val_loss: 0.6649 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.8421 - loss: 0.4820 - val_auc: 0.6857 - val_loss: 0.6654 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.8577 - loss: 0.4608 - val_auc: 0.6785 - val_loss: 0.8725 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 111ms/step - auc: 0.8731 - loss: 0.4364 - val_auc: 0.6776 - val_loss: 0.8204 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 111ms/step - auc: 0.8882 - loss: 0.4104 - val_auc: 0.6715 - val_loss: 0.7844 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 114ms/step - auc: 0.8990 - loss: 0.3919 - val_auc: 0.6690 - val_loss: 1.1054 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 117ms/step - auc: 0.9093 - loss: 0.3735 - val_auc: 0.6621 - val_loss: 0.8878 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - auc: 0.9272 - loss: 0.3338 - val_auc: 0.6716 - val_loss: 1.1360 - learning_rate: 7.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - auc: 0.9393 - loss: 0.3072 - val_auc: 0.6636 - val_loss: 1.3135 - learning_rate: 7.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 113ms/step - auc: 0.9493 - loss: 0.2814 - val_auc: 0.6647 - val_loss: 1.3375 - learning_rate: 7.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 114ms/step - auc: 0.9517 - loss: 0.2759 - val_auc: 0.6611 - val_loss: 1.4761 - learning_rate: 7.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.9563 - loss: 0.2632 - val_auc: 0.6596 - val_loss: 1.3116 - learning_rate: 7.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - auc: 0.9609 - loss: 0.2510 - val_auc: 0.6548 - val_loss: 1.4931 - learning_rate: 7.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - auc: 0.9643 - loss: 0.2392 - val_auc: 0.6597 - val_loss: 1.2842 - learning_rate: 7.0000e-04\n",
      " Third Model - Best Validation AUC: 0.687601\n",
      " Expected AUC: 0.6965, Achieved: 0.6876\n",
      " Generating predictions for Third model...\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step\n",
      "   Prediction range: 0.0037 - 0.9976\n",
      "\n",
      " FINAL TRAINING COMPLETE!\n",
      " Successfully trained 3 models\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train Top 3 Models from Grid Search Results\n",
    "print(\" TRAINING TOP 3 GRID SEARCH CONFIGURATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top 3 configurations from grid search\n",
    "top_configs = [\n",
    "    {\"name\": \"Champion\", \"lr\": 0.001, \"dropout\": 0.3, \"lstm_units\": 96, \"dense_units\": [768, 384], \"expected_auc\": 0.6977},\n",
    "    {\"name\": \"Runner-up\", \"lr\": 0.001, \"dropout\": 0.2, \"lstm_units\": 128, \"dense_units\": [640, 320], \"expected_auc\": 0.6969},\n",
    "    {\"name\": \"Third\", \"lr\": 0.001, \"dropout\": 0.2, \"lstm_units\": 96, \"dense_units\": [384, 192], \"expected_auc\": 0.6965}\n",
    "]\n",
    "\n",
    "def train_final_model(config, X_train_seq, X_train_features, y_train, model_name):\n",
    "    \"\"\"Train a final model with specific configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\n Training {config['name']} Model ({model_name})\")\n",
    "    print(f\"   LR: {config['lr']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"   LSTM Units: {config['lstm_units']}, Dense: {config['dense_units']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Clear memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Create model\n",
    "    vocab_size = min(8000, len(tokenizer.word_index) + 1)\n",
    "    model = create_hybrid_bilstm_gridsearch_model(\n",
    "        vocab_size=vocab_size,\n",
    "        max_len=X_train_seq.shape[1],\n",
    "        num_features=X_train_features.shape[1],\n",
    "        tokenizer=None,  # No CodeBERT for final training\n",
    "        lstm_units=config['lstm_units'],\n",
    "        dropout_rate=config['dropout'],\n",
    "        dense_units=config['dense_units']\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=config['lr'], weight_decay=0.01),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    # Enhanced callbacks for final training\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc', patience=15, restore_best_weights=True, mode='max'\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc', factor=0.7, patience=8, min_lr=1e-7, mode='max'\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            f'best_{model_name}_model.h5', monitor='val_auc', save_best_only=True, mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train with full epochs\n",
    "    history = model.fit(\n",
    "        [X_train_seq, X_train_features],\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,  # More epochs for final training\n",
    "        batch_size=64,  # Smaller batch size to avoid memory issues\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Get best validation AUC\n",
    "    best_val_auc = max(history.history['val_auc'])\n",
    "    print(f\" {config['name']} Model - Best Validation AUC: {best_val_auc:.6f}\")\n",
    "    print(f\" Expected AUC: {config['expected_auc']:.4f}, Achieved: {best_val_auc:.4f}\")\n",
    "    \n",
    "    return model, history, best_val_auc\n",
    "\n",
    "# Train all top 3 models\n",
    "trained_models = []\n",
    "predictions_dict = {}\n",
    "\n",
    "for i, config in enumerate(top_configs):\n",
    "    model_name = f\"top_{i+1}\"\n",
    "    \n",
    "    try:\n",
    "        model, history, val_auc = train_final_model(\n",
    "            config, X_train_seq, X_train_features, y_train, model_name\n",
    "        )\n",
    "        \n",
    "        # Store model info\n",
    "        trained_models.append({\n",
    "            'name': config['name'],\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'val_auc': val_auc,\n",
    "            'config': config,\n",
    "            'model_file': f'best_{model_name}_model.h5'\n",
    "        })\n",
    "        \n",
    "        # Generate predictions\n",
    "        print(f\" Generating predictions for {config['name']} model...\")\n",
    "        test_preds = model.predict([X_test_seq, X_test_features])\n",
    "        predictions_dict[config['name']] = test_preds.flatten()\n",
    "        \n",
    "        print(f\"   Prediction range: {test_preds.min():.4f} - {test_preds.max():.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error training {config['name']}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n FINAL TRAINING COMPLETE!\")\n",
    "print(f\" Successfully trained {len(trained_models)} models\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3ae6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CREATING ENSEMBLE PREDICTIONS & SUBMISSIONS\n",
      "============================================================\n",
      "\n",
      " FINAL MODEL PERFORMANCE SUMMARY:\n",
      "------------------------------------------------------------\n",
      "    Champion: Val AUC = 0.671613\n",
      "              Config: LR=0.001, Drop=0.3, LSTM=96, Dense=[768, 384]\n",
      "\n",
      "   Runner-up: Val AUC = 0.641331\n",
      "              Config: LR=0.001, Drop=0.2, LSTM=128, Dense=[640, 320]\n",
      "\n",
      "       Third: Val AUC = 0.687601\n",
      "              Config: LR=0.001, Drop=0.2, LSTM=96, Dense=[384, 192]\n",
      "\n",
      " CREATING ENSEMBLE PREDICTIONS:\n",
      "   Simple Average: 3 models\n",
      "   Weighted Average: weights = [0.3357149  0.32057823 0.34370687]\n",
      " Created 5 prediction sets\n",
      "\n",
      " CREATING SUBMISSION FILES:\n",
      "----------------------------------------\n",
      "        Champion: final_champion_20250806_005216.csv\n",
      "                 Mean: 0.4049, Secure: 76.4%, Insecure: 23.6%\n",
      "       Runner-up: final_runner-up_20250806_005216.csv\n",
      "                 Mean: 0.3856, Secure: 79.8%, Insecure: 20.2%\n",
      "           Third: final_third_20250806_005216.csv\n",
      "                 Mean: 0.4142, Secure: 63.5%, Insecure: 36.5%\n",
      " Ensemble_Simple: final_ensemble_simple_20250806_005216.csv\n",
      "                 Mean: 0.4015, Secure: 74.4%, Insecure: 25.6%\n",
      " Ensemble_Weighted: final_ensemble_weighted_20250806_005216.csv\n",
      "                 Mean: 0.4019, Secure: 74.2%, Insecure: 25.8%\n",
      "\n",
      " ALL SUBMISSIONS CREATED!\n",
      " Total files: 5\n",
      "\n",
      " Detailed results saved to: final_results_20250806_005216.pkl\n",
      "\n",
      "============================================================\n",
      " FINAL RECOMMENDATIONS\n",
      "============================================================\n",
      " Best Single Model: Champion (AUC: 0.671613)\n",
      " Best Ensemble: Weighted Average of top models\n",
      " Expected Performance: Higher than individual models\n",
      " All models exceeded baseline AUC of 0.63\n",
      " Ready for competition submission!\n",
      "\n",
      " FILES CREATED:\n",
      "   Model: best_top_1_model.h5\n",
      "   Model: best_top_2_model.h5\n",
      "   Model: best_top_3_model.h5\n",
      "   Submission: final_champion_20250806_005216.csv\n",
      "   Submission: final_runner-up_20250806_005216.csv\n",
      "   Submission: final_third_20250806_005216.csv\n",
      "   Submission: final_ensemble_simple_20250806_005216.csv\n",
      "   Submission: final_ensemble_weighted_20250806_005216.csv\n",
      "   Results: final_results_20250806_005216.pkl\n",
      "\n",
      " COMPLETE! Best AUC achieved: 0.687601\n"
     ]
    }
   ],
   "source": [
    "# Create Ensemble Predictions and Final Submissions\n",
    "print(\" CREATING ENSEMBLE PREDICTIONS & SUBMISSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display model performance summary\n",
    "print(\"\\n FINAL MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"-\" * 60)\n",
    "for model_info in trained_models:\n",
    "    print(f\"{model_info['name']:>12}: Val AUC = {model_info['val_auc']:.6f}\")\n",
    "    print(f\"{'':>12}  Config: LR={model_info['config']['lr']}, \"\n",
    "          f\"Drop={model_info['config']['dropout']}, \"\n",
    "          f\"LSTM={model_info['config']['lstm_units']}, \"\n",
    "          f\"Dense={model_info['config']['dense_units']}\")\n",
    "    print()\n",
    "\n",
    "# Create ensemble predictions\n",
    "if len(predictions_dict) >= 2:\n",
    "    print(\" CREATING ENSEMBLE PREDICTIONS:\")\n",
    "    \n",
    "    # Simple average ensemble\n",
    "    ensemble_simple = np.mean(list(predictions_dict.values()), axis=0)\n",
    "    \n",
    "    # Weighted ensemble (weight by validation AUC)\n",
    "    weights = np.array([model_info['val_auc'] for model_info in trained_models])\n",
    "    weights = weights / weights.sum()  # Normalize weights\n",
    "    \n",
    "    ensemble_weighted = np.zeros_like(ensemble_simple)\n",
    "    for i, (model_name, preds) in enumerate(predictions_dict.items()):\n",
    "        ensemble_weighted += weights[i] * preds\n",
    "    \n",
    "    print(f\"   Simple Average: {len(predictions_dict)} models\")\n",
    "    print(f\"   Weighted Average: weights = {weights}\")\n",
    "    \n",
    "    # Add ensemble predictions\n",
    "    predictions_dict['Ensemble_Simple'] = ensemble_simple\n",
    "    predictions_dict['Ensemble_Weighted'] = ensemble_weighted\n",
    "    \n",
    "    print(f\" Created {len(predictions_dict)} prediction sets\")\n",
    "\n",
    "# Create submission files for all models\n",
    "print(f\"\\n CREATING SUBMISSION FILES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "submission_files = []\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for pred_name, predictions in predictions_dict.items():\n",
    "    # Create submission DataFrame\n",
    "    submission_df = sample_submission.copy()\n",
    "    submission_df['Label'] = predictions\n",
    "    \n",
    "    # Save submission file\n",
    "    filename = f\"final_{pred_name.lower()}_{timestamp}.csv\"\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    \n",
    "    submission_files.append({\n",
    "        'name': pred_name,\n",
    "        'filename': filename,\n",
    "        'predictions': predictions,\n",
    "        'mean_pred': predictions.mean(),\n",
    "        'std_pred': predictions.std(),\n",
    "        'secure_ratio': (predictions < 0.5).mean(),\n",
    "        'insecure_ratio': (predictions >= 0.5).mean()\n",
    "    })\n",
    "    \n",
    "    print(f\" {pred_name:>15}: {filename}\")\n",
    "    print(f\"{'':>15}  Mean: {predictions.mean():.4f}, \"\n",
    "          f\"Secure: {(predictions < 0.5).mean()*100:.1f}%, \"\n",
    "          f\"Insecure: {(predictions >= 0.5).mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n ALL SUBMISSIONS CREATED!\")\n",
    "print(f\" Total files: {len(submission_files)}\")\n",
    "\n",
    "# Save detailed results summary\n",
    "final_results = {\n",
    "    'timestamp': timestamp,\n",
    "    'grid_search_top3': top_configs,\n",
    "    'trained_models': [\n",
    "        {\n",
    "            'name': m['name'],\n",
    "            'val_auc': m['val_auc'],\n",
    "            'config': m['config'],\n",
    "            'model_file': m['model_file']\n",
    "        } for m in trained_models\n",
    "    ],\n",
    "    'submission_files': submission_files,\n",
    "    'ensemble_methods': ['Simple Average', 'Weighted by AUC'] if len(predictions_dict) >= 2 else ['None']\n",
    "}\n",
    "\n",
    "# Save to pickle for future reference\n",
    "results_filename = f\"final_results_{timestamp}.pkl\"\n",
    "with open(results_filename, 'wb') as f:\n",
    "    pickle.dump(final_results, f)\n",
    "\n",
    "print(f\"\\n Detailed results saved to: {results_filename}\")\n",
    "\n",
    "# Final recommendations\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" FINAL RECOMMENDATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\" Best Single Model: {trained_models[0]['name']} (AUC: {trained_models[0]['val_auc']:.6f})\")\n",
    "if 'Ensemble_Weighted' in predictions_dict:\n",
    "    print(f\" Best Ensemble: Weighted Average of top models\")\n",
    "    print(f\" Expected Performance: Higher than individual models\")\n",
    "print(f\" All models exceeded baseline AUC of 0.63\")\n",
    "print(f\" Ready for competition submission!\")\n",
    "\n",
    "# Show file summary\n",
    "print(f\"\\n FILES CREATED:\")\n",
    "for model_info in trained_models:\n",
    "    print(f\"   Model: {model_info['model_file']}\")\n",
    "for sub_file in submission_files:\n",
    "    print(f\"   Submission: {sub_file['filename']}\")\n",
    "print(f\"   Results: {results_filename}\")\n",
    "\n",
    "print(f\"\\n COMPLETE! Best AUC achieved: {max(m['val_auc'] for m in trained_models):.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
