{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93efcad6",
   "metadata": {},
   "source": [
    "## ISE: Defect Detection Challenge\n",
    "#### Description\n",
    "In this competition, your task is to develop a model that can accurately classify source code snippets as either secure or insecure. With the rise of software vulnerabilities like resource leaks, use-after-free vulnerabilities, and denial-of-service (DoS) attacks, identifying insecure code is crucial for maintaining robust software systems.\n",
    "Participants will be provided with a dataset containing labeled code snippets. The labels indicate whether the code is secure (0) or insecure (1). Your goal is to create an effective machine learning model that can predict these labels with high accuracy.\n",
    "#### Key Objectives \n",
    "- Analyze code snippets for potential vulnerabilities.\n",
    "- Develop models to automate the classification of secure and insecure code.\n",
    "- Ensure the ROC score exceeds 0.63."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d0c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98751e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (20000, 3)\n",
      "Test data shape: (7000, 2)\n",
      "\n",
      "Training data columns: ['ID', 'code', 'Label']\n",
      "   ID                                               code  Label\n",
      "0   0  int page_check_range(target_ulong start, targe...      0\n",
      "1   1  static void pxa2xx_lcdc_dma0_redraw_rot0(PXA2x...      0\n",
      "2   2  void OPPROTO op_POWER_slq (void)\\n\\n{\\n\\n    u...      1\n",
      "ID       0\n",
      "code     0\n",
      "Label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# data info\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
    "\n",
    "print(train_df.head(3))\n",
    "print(train_df.isnull().sum()) # none num \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509e62c",
   "metadata": {},
   "source": [
    "## C++ Code Preprocessing Pipeline \n",
    "- Basic Text Cleaning \n",
    "- C++ Specific Normalization\n",
    "- Features Enginerring \n",
    "- Tokenization\n",
    "- Vectorization using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc246a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_cpp_code(code):\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "    \n",
    "    # Remove multi-line comments  \n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove string literals (but keep structure)\n",
    "    code = re.sub(r'\"[^\"]*\"', '\"STRING\"', code)\n",
    "    code = re.sub(r\"'[^']*'\", \"'CHAR'\", code)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    code = re.sub(r'\\s+', ' ', code)\n",
    "    code = code.strip()\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cpp_code(code):\n",
    "    # Normalize variable names \n",
    "    code = re.sub(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', \n",
    "                  lambda m: normalize_identifier(m.group()), code)\n",
    "    \n",
    "    # Normalize numeric literals\n",
    "    code = re.sub(r'\\b\\d+\\b', 'NUM', code)\n",
    "    code = re.sub(r'\\b0x[0-9a-fA-F]+\\b', 'HEX', code)\n",
    "    \n",
    "    # Normalize function calls \n",
    "    code = re.sub(r'(\\w+)\\s*\\(', r'FUNC(', code)\n",
    "    \n",
    "    return code\n",
    "\n",
    "def normalize_identifier(name):\n",
    "    # Keep important C++ keywords and functions\n",
    "    cpp_keywords = {'int', 'char', 'void', 'if', 'else', 'for', 'while', \n",
    "                   'malloc', 'free', 'strcpy', 'strlen', 'memcpy', 'sizeof'}\n",
    "    \n",
    "    if name.lower() in cpp_keywords:\n",
    "        return name\n",
    "    elif len(name) <= 3:\n",
    "        return name  # Keep short vars\n",
    "    else:\n",
    "        return 'VAR'  # Generic variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_security_features(code):\n",
    "    features = {}\n",
    "    \n",
    "    # Enhanced dangerous function calls with more comprehensive list\n",
    "    dangerous_funcs = ['strcpy', 'strcat', 'gets', 'sprintf', 'scanf', \n",
    "                      'malloc', 'free', 'memcpy', 'strncpy', 'strncat',\n",
    "                      'vsprintf', 'vsnprintf', 'sscanf', 'fscanf', 'fgets',\n",
    "                      'alloca', 'realloc', 'calloc']\n",
    "    \n",
    "    for func in dangerous_funcs:\n",
    "        features[f'has_{func}'] = int(func in code.lower())\n",
    "    \n",
    "    # Memory operations \n",
    "    features['ptr_operations'] = len(re.findall(r'\\*|\\&', code))\n",
    "    features['array_access'] = len(re.findall(r'\\[.*?\\]', code))\n",
    "    features['memory_alloc'] = len(re.findall(r'malloc|calloc|new|alloca|realloc', code, re.IGNORECASE))\n",
    "    features['memory_free'] = len(re.findall(r'free|delete', code, re.IGNORECASE))\n",
    "    \n",
    "    # Advanced vulnerability patterns\n",
    "    features['buffer_overflow_risk'] = len(re.findall(r'(strcpy|strcat|gets|sprintf)\\s*\\(', code, re.IGNORECASE))\n",
    "    features['format_string_vuln'] = len(re.findall(r'printf\\s*\\(\\s*[a-zA-Z_]\\w*\\s*[,)]', code))\n",
    "    features['use_after_free_risk'] = detect_use_after_free_pattern(code)\n",
    "    features['double_free_risk'] = detect_double_free_pattern(code)\n",
    "    features['memory_leak_risk'] = abs(features['memory_alloc'] - features['memory_free'])\n",
    "    \n",
    "    # Input validation issues\n",
    "    features['unchecked_input'] = len(re.findall(r'(scanf|gets|fgets)\\s*\\(', code, re.IGNORECASE))\n",
    "    features['missing_null_check'] = detect_missing_null_checks(code)\n",
    "    features['array_bounds_risk'] = detect_array_bounds_issues(code)\n",
    "    \n",
    "    # Integer overflow/underflow risks\n",
    "    features['integer_overflow_risk'] = len(re.findall(r'(\\+\\+|\\-\\-|\\+=|\\-=|\\*=).*?(\\[|\\*)', code))\n",
    "    features['signed_unsigned_mix'] = len(re.findall(r'(unsigned|signed)\\s+\\w+.*?(signed|unsigned)', code, re.IGNORECASE))\n",
    "    \n",
    "    # Control flow complexity (enhanced)\n",
    "    features['if_statements'] = len(re.findall(r'\\bif\\b', code))\n",
    "    features['nested_loops'] = detect_nested_complexity(code, r'\\b(for|while)\\b')\n",
    "    features['switch_statements'] = len(re.findall(r'\\bswitch\\b', code))\n",
    "    features['goto_statements'] = len(re.findall(r'\\bgoto\\b', code))\n",
    "    features['function_calls'] = len(re.findall(r'\\w+\\s*\\(', code))\n",
    "    \n",
    "    # Code quality indicators\n",
    "    features['magic_numbers'] = len(re.findall(r'\\b\\d{2,}\\b', code))\n",
    "    features['long_functions'] = int(len(code.split('\\n')) > 50)\n",
    "    features['deep_nesting'] = calculate_max_nesting_depth(code)\n",
    "    features['cyclomatic_complexity'] = estimate_cyclomatic_complexity(code)\n",
    "    \n",
    "    # String and file operations\n",
    "    features['string_operations'] = len(re.findall(r'str(cpy|cat|cmp|len|chr|str)', code, re.IGNORECASE))\n",
    "    features['file_operations'] = len(re.findall(r'(fopen|fclose|fread|fwrite|fprintf)', code, re.IGNORECASE))\n",
    "    \n",
    "    # Pointer arithmetic and casting\n",
    "    features['pointer_arithmetic'] = len(re.findall(r'(\\*\\s*\\w+\\s*[\\+\\-]|\\w+\\s*[\\+\\-]\\s*\\d+\\s*\\))', code))\n",
    "    features['type_casting'] = len(re.findall(r'\\([a-zA-Z_]\\w*\\s*\\*?\\s*\\)', code))\n",
    "    features['void_pointer_usage'] = len(re.findall(r'void\\s*\\*', code, re.IGNORECASE))\n",
    "    \n",
    "    # Security-specific patterns\n",
    "    features['hardcoded_values'] = detect_hardcoded_credentials(code)\n",
    "    features['privilege_operations'] = len(re.findall(r'(setuid|setgid|chmod|chown|su|sudo)', code, re.IGNORECASE))\n",
    "    features['system_calls'] = len(re.findall(r'(system|exec|popen|fork)', code, re.IGNORECASE))\n",
    "    \n",
    "    # Statistical features\n",
    "    features['code_length'] = len(code)\n",
    "    features['line_count'] = len(code.split('\\n'))\n",
    "    features['avg_line_length'] = features['code_length'] / max(1, features['line_count'])\n",
    "    features['char_entropy'] = calculate_entropy(code)\n",
    "    features['unique_char_ratio'] = len(set(code.lower())) / max(1, len(code))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefae8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for advanced vulnerability detection\n",
    "def detect_use_after_free_pattern(code):\n",
    "    \"\"\"Detect potential use-after-free patterns\"\"\"\n",
    "    patterns = [\n",
    "        r'free\\s*\\([^)]+\\).*?\\*\\s*\\w+',  # free followed by dereference\n",
    "        r'delete\\s+\\w+.*?\\w+\\s*\\[',      # delete followed by array access\n",
    "        r'free\\s*\\([^)]+\\).*?\\w+\\s*\\(',  # free followed by function call with same var\n",
    "    ]\n",
    "    \n",
    "    count = 0\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, code, re.DOTALL | re.IGNORECASE)\n",
    "        count += len(matches)\n",
    "    \n",
    "    return count\n",
    "\n",
    "def detect_double_free_pattern(code):\n",
    "    \"\"\"Detect potential double free patterns\"\"\"\n",
    "    free_calls = re.findall(r'free\\s*\\(\\s*(\\w+)\\s*\\)', code, re.IGNORECASE)\n",
    "    if len(free_calls) != len(set(free_calls)):\n",
    "        return 1  # Potential double free\n",
    "    return 0\n",
    "\n",
    "def detect_missing_null_checks(code):\n",
    "    \"\"\"Detect pointer usage without null checks\"\"\"\n",
    "    ptr_usage = len(re.findall(r'\\*\\s*\\w+', code))\n",
    "    null_checks = len(re.findall(r'if\\s*\\(\\s*\\w+\\s*[!=]=\\s*NULL\\s*\\)', code, re.IGNORECASE))\n",
    "    return max(0, ptr_usage - null_checks)\n",
    "\n",
    "def detect_array_bounds_issues(code):\n",
    "    \"\"\"Detect array access without bounds checking\"\"\"\n",
    "    array_access = re.findall(r'\\w+\\s*\\[\\s*([^]]+)\\s*\\]', code)\n",
    "    bounds_checks = len(re.findall(r'if\\s*\\([^)]*(<|>|<=|>=)[^)]*\\)', code))\n",
    "    return max(0, len(array_access) - bounds_checks)\n",
    "\n",
    "def detect_nested_complexity(code, pattern):\n",
    "    \"\"\"Detect nested control structures\"\"\"\n",
    "    lines = code.split('\\n')\n",
    "    max_nested = 0\n",
    "    current_nested = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if re.search(pattern, line):\n",
    "            current_nested += 1\n",
    "            max_nested = max(max_nested, current_nested)\n",
    "        if '}' in line:\n",
    "            current_nested = max(0, current_nested - 1)\n",
    "    \n",
    "    return max_nested\n",
    "\n",
    "def calculate_max_nesting_depth(code):\n",
    "    \"\"\"Calculate maximum nesting depth\"\"\"\n",
    "    depth = 0\n",
    "    max_depth = 0\n",
    "    \n",
    "    for char in code:\n",
    "        if char == '{':\n",
    "            depth += 1\n",
    "            max_depth = max(max_depth, depth)\n",
    "        elif char == '}':\n",
    "            depth = max(0, depth - 1)\n",
    "    \n",
    "    return max_depth\n",
    "\n",
    "def estimate_cyclomatic_complexity(code):\n",
    "    \"\"\"Estimate cyclomatic complexity\"\"\"\n",
    "    decision_points = ['if', 'else', 'elif', 'for', 'while', 'case', 'catch', '\\?', '&&', '\\|\\|']\n",
    "    complexity = 1  # Base complexity\n",
    "    \n",
    "    for keyword in decision_points:\n",
    "        complexity += len(re.findall(rf'\\b{keyword}\\b', code, re.IGNORECASE))\n",
    "    \n",
    "    return complexity\n",
    "\n",
    "def detect_hardcoded_credentials(code):\n",
    "    \"\"\"Detect hardcoded passwords, keys, etc.\"\"\"\n",
    "    patterns = [\n",
    "        r'(password|passwd|pwd)\\s*=\\s*[\"\\'][^\"\\']{3,}[\"\\']',\n",
    "        r'(key|secret|token)\\s*=\\s*[\"\\'][^\"\\']{8,}[\"\\']',\n",
    "        r'(api_key|apikey)\\s*=\\s*[\"\\'][^\"\\']{10,}[\"\\']',\n",
    "    ]\n",
    "    \n",
    "    count = 0\n",
    "    for pattern in patterns:\n",
    "        count += len(re.findall(pattern, code, re.IGNORECASE))\n",
    "    \n",
    "    return count\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"Calculate Shannon entropy of text\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    char_counts = {}\n",
    "    for char in text.lower():\n",
    "        char_counts[char] = char_counts.get(char, 0) + 1\n",
    "    \n",
    "    entropy = 0\n",
    "    text_len = len(text)\n",
    "    \n",
    "    for count in char_counts.values():\n",
    "        probability = count / text_len\n",
    "        if probability > 0:\n",
    "            entropy -= probability * np.log2(probability)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256450b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cpp_dataset(df):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for C++ code dataset with enhanced features\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    print(f\"Processing {len(df)} code samples...\")\n",
    "    \n",
    "    print(\"Step 1: Cleaning code...\")\n",
    "    processed_df['cleaned_code'] = processed_df['code'].apply(clean_cpp_code)\n",
    "    \n",
    "    print(\"Step 2: Normalizing code...\")\n",
    "    processed_df['normalized_code'] = processed_df['cleaned_code'].apply(normalize_cpp_code)\n",
    "    \n",
    "    print(\"Step 3: Extracting enhanced security features...\")\n",
    "    security_features_list = []\n",
    "    \n",
    "    for idx, code in enumerate(processed_df['code']):\n",
    "        if idx % 5000 == 0:\n",
    "            print(f\"  Processing sample {idx}/{len(processed_df)}\")\n",
    "        \n",
    "        features = extract_security_features(code) \n",
    "        security_features_list.append(features)\n",
    "    \n",
    "    security_df = pd.DataFrame(security_features_list)\n",
    "    \n",
    "    result_df = pd.concat([\n",
    "        processed_df[['normalized_code']],  \n",
    "        security_df,  \n",
    "    ], axis=1)\n",
    "    \n",
    "    if 'Label' in processed_df.columns:\n",
    "        result_df['Label'] = processed_df['Label']\n",
    "    \n",
    "    print(f\"Preprocessing complete!\")\n",
    "    print(f\"Enhanced features created: {len(security_df.columns)} features\")\n",
    "    print(f\"Final shape: {result_df.shape}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54837cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "def augment_features(X, y, noise_factor=0.1, augment_ratio=0.5):\n",
    "    \"\"\"Add gaussian noise to numerical features\"\"\"\n",
    "    n_samples = int(len(X) * augment_ratio)\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    \n",
    "    X_aug = X[indices].copy()\n",
    "    y_aug = y[indices].copy()\n",
    "    \n",
    "    # Add noise to TF-IDF features (first part of feature vector)\n",
    "    tfidf_end = 2000  # Adjust based on your TF-IDF feature count\n",
    "    X_aug[:, tfidf_end:] += np.random.normal(0, noise_factor, X_aug[:, tfidf_end:].shape)\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    X_combined = np.vstack([X, X_aug])\n",
    "    y_combined = np.hstack([y, y_aug])\n",
    "    \n",
    "    # Shuffle\n",
    "    shuffle_idx = np.random.permutation(len(X_combined))\n",
    "    return X_combined[shuffle_idx], y_combined[shuffle_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d898313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced features...\n",
      "Step 1: Applying enhanced preprocessing...\n",
      "Processing 20000 code samples...\n",
      "Step 1: Cleaning code...\n",
      "Step 2: Normalizing code...\n",
      "Step 3: Extracting enhanced security features...\n",
      "  Processing sample 0/20000\n",
      "  Processing sample 5000/20000\n",
      "  Processing sample 10000/20000\n",
      "  Processing sample 15000/20000\n",
      "Preprocessing complete!\n",
      "Enhanced features created: 54 features\n",
      "Final shape: (20000, 56)\n",
      "Processing 7000 code samples...\n",
      "Step 1: Cleaning code...\n",
      "Step 2: Normalizing code...\n",
      "Step 3: Extracting enhanced security features...\n",
      "  Processing sample 0/7000\n",
      "  Processing sample 5000/7000\n",
      "Preprocessing complete!\n",
      "Enhanced features created: 54 features\n",
      "Final shape: (7000, 55)\n",
      "Step 2: Creating TF-IDF features...\n",
      "Step 3: Using enhanced security features...\n",
      "Total enhanced features: 54\n",
      "Feature categories:\n",
      "  - Dangerous function detection: 18\n",
      "  - Vulnerability patterns: 7\n",
      "  - Code complexity: 2\n",
      "  - Security patterns: 3\n",
      "Final shapes - Train: (20000, 2054), Test: (7000, 2054)\n",
      "TF-IDF features: 2000\n",
      "Enhanced security features: 54\n",
      "Total features: 2054\n",
      "\n",
      "Class distribution:\n",
      "  Class 0: 10878 samples (54.4%)\n",
      "  Class 1: 9122 samples (45.6%)\n"
     ]
    }
   ],
   "source": [
    "def create_pipeline_features(train_df, test_df):\n",
    "    \"\"\"Enhanced pipeline using your existing preprocessing with advanced features\"\"\"\n",
    "    \n",
    "    print(\"Step 1: Applying enhanced preprocessing...\")\n",
    "\n",
    "    train_processed = preprocess_cpp_dataset(train_df.copy())\n",
    "    test_processed = preprocess_cpp_dataset(test_df.copy())\n",
    "    \n",
    "    print(\"Step 2: Creating TF-IDF features...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=2000,  \n",
    "        ngram_range=(1, 3),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "        stop_words='english'  \n",
    "    )\n",
    "    \n",
    "    tfidf_train = vectorizer.fit_transform(train_processed['normalized_code'])\n",
    "    tfidf_test = vectorizer.transform(test_processed['normalized_code'])\n",
    "    \n",
    "    print(\"Step 3: Using enhanced security features...\")\n",
    "    \n",
    "    feature_cols = [col for col in train_processed.columns \n",
    "                   if col not in ['normalized_code', 'Label']]\n",
    "    \n",
    "    print(f\"Total enhanced features: {len(feature_cols)}\")\n",
    "    print(\"Feature categories:\")\n",
    "    print(f\"  - Dangerous function detection: {len([f for f in feature_cols if f.startswith('has_')])}\")\n",
    "    print(f\"  - Vulnerability patterns: {len([f for f in feature_cols if 'risk' in f or 'vuln' in f])}\")\n",
    "    print(f\"  - Code complexity: {len([f for f in feature_cols if any(x in f for x in ['complexity', 'nesting', 'depth'])])}\")\n",
    "    print(f\"  - Security patterns: {len([f for f in feature_cols if any(x in f for x in ['hardcoded', 'privilege', 'system'])])}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_numerical = scaler.fit_transform(train_processed[feature_cols].fillna(0))\n",
    "    test_numerical = scaler.transform(test_processed[feature_cols].fillna(0))\n",
    "    \n",
    "    X_train = np.hstack([\n",
    "        tfidf_train.toarray(),\n",
    "        train_numerical\n",
    "    ])\n",
    "    \n",
    "    X_test = np.hstack([\n",
    "        tfidf_test.toarray(),\n",
    "        test_numerical\n",
    "    ])\n",
    "    \n",
    "    y_train = train_processed['Label'].values\n",
    "    \n",
    "    print(f\"Final shapes - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"TF-IDF features: {tfidf_train.shape[1]}\")\n",
    "    print(f\"Enhanced security features: {len(feature_cols)}\")\n",
    "    print(f\"Total features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train\n",
    "\n",
    "print(\"Creating enhanced features...\")\n",
    "X_train, X_test, y_train = create_pipeline_features(train_df, test_df)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count} samples ({count/len(y_train)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb2c65",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_residual_block(x, units, dropout_rate=0.3):\n",
    "    # Main path\n",
    "    main = layers.Dense(units, activation='relu')(x)\n",
    "    main = layers.BatchNormalization()(main)\n",
    "    main = layers.Dropout(dropout_rate)(main)\n",
    "    main = layers.Dense(units)(main)\n",
    "    main = layers.BatchNormalization()(main)\n",
    "    \n",
    "    # skip connection\n",
    "    if x.shape[-1] == units:\n",
    "        skip = x\n",
    "    else:\n",
    "        skip = layers.Dense(units)(x)\n",
    "    \n",
    "    # combine and activate\n",
    "    output = layers.Add()([main, skip])\n",
    "    output = layers.Activation('relu')(output)\n",
    "    output = layers.Dropout(dropout_rate)(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def res_net(X_tr, X_val, y_tr, y_val):\n",
    "    print(\"Training Residual Network...\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        input_layer = layers.Input(shape=(X_tr.shape[1],))\n",
    "\n",
    "        x = layers.Dense(4096, activation='relu')(input_layer)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        x = layers.Dense(3072, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        \n",
    "        x = layers.Dense(2048, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = create_residual_block(x, 2048, 0.4)\n",
    "        x = create_residual_block(x, 1024, 0.45)\n",
    "        x = create_residual_block(x, 1024, 0.5)\n",
    "        x = create_residual_block(x, 512, 0.6)\n",
    "        x = create_residual_block(x, 256, 0.55)\n",
    "        x = create_residual_block(x, 128, 0.4)\n",
    "\n",
    "        x = layers.Dense(96, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.35)(x)\n",
    "        \n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        \n",
    "        x = layers.Dense(16, activation='relu')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "\n",
    "        output = layers.Dense(1, activation='sigmoid',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "\n",
    "        nn_model = keras.Model(inputs=input_layer, outputs=output)   \n",
    "    nn_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=[keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, min_lr=1e-6, mode='max')\n",
    "\n",
    "    nn_model.fit(\n",
    "        X_tr, y_tr, \n",
    "        epochs=100, \n",
    "        batch_size=128, \n",
    "        validation_data=(X_val, y_val), \n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    nn_train_pred = nn_model.predict(X_tr)\n",
    "    nn_train_score = roc_auc_score(y_tr, nn_train_pred)\n",
    "    print(f\"   ResNet ROC-AUC on train: {nn_train_score:.4f}\")\n",
    "    \n",
    "    nn_pred = nn_model.predict(X_val)\n",
    "    nn_score = roc_auc_score(y_val, nn_pred)\n",
    "    print(f\"   ResNet ROC-AUC: {nn_score:.4f}\")\n",
    "\n",
    "    return nn_model, nn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8bf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network...\n",
      "Epoch 1/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 80ms/step - auc: 0.5139 - loss: 0.7996 - val_auc: 0.5407 - val_loss: 0.6946 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.5177 - loss: 0.7253 - val_auc: 0.5528 - val_loss: 0.6870 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.5317 - loss: 0.7007 - val_auc: 0.5607 - val_loss: 0.6802 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.5548 - loss: 0.6840 - val_auc: 0.5740 - val_loss: 0.6764 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.5775 - loss: 0.6736 - val_auc: 0.5884 - val_loss: 0.6682 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.6151 - loss: 0.6587 - val_auc: 0.6089 - val_loss: 0.6624 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.6601 - loss: 0.6419 - val_auc: 0.6225 - val_loss: 0.6641 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.6900 - loss: 0.6232 - val_auc: 0.6331 - val_loss: 0.6547 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.7191 - loss: 0.6040 - val_auc: 0.6411 - val_loss: 0.6615 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.7424 - loss: 0.5857 - val_auc: 0.6413 - val_loss: 0.6789 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.7696 - loss: 0.5643 - val_auc: 0.6488 - val_loss: 0.6728 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.7843 - loss: 0.5494 - val_auc: 0.6353 - val_loss: 0.6850 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.8020 - loss: 0.5310 - val_auc: 0.6432 - val_loss: 0.6890 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.8163 - loss: 0.5139 - val_auc: 0.6213 - val_loss: 0.7324 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.8321 - loss: 0.4943 - val_auc: 0.6401 - val_loss: 0.7452 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.8426 - loss: 0.4807 - val_auc: 0.6461 - val_loss: 0.7318 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - auc: 0.8770 - loss: 0.4291 - val_auc: 0.6352 - val_loss: 0.8442 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.8896 - loss: 0.4084 - val_auc: 0.6173 - val_loss: 0.9376 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.8962 - loss: 0.3984 - val_auc: 0.6274 - val_loss: 0.9266 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.9033 - loss: 0.3828 - val_auc: 0.6264 - val_loss: 0.9621 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - auc: 0.9085 - loss: 0.3757 - val_auc: 0.6264 - val_loss: 0.9860 - learning_rate: 5.0000e-04\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step\n",
      "   Neural Network ROC-AUC on train: 0.8270\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "   Neural Network ROC-AUC: 0.6487\n",
      "Training Residual Network...\n",
      "Epoch 1/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 170ms/step - auc: 0.5054 - loss: 0.7590 - val_auc: 0.4502 - val_loss: 0.6972 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 165ms/step - auc: 0.4977 - loss: 0.7105 - val_auc: 0.4609 - val_loss: 0.6949 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 171ms/step - auc: 0.5056 - loss: 0.6996 - val_auc: 0.4736 - val_loss: 0.6929 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 175ms/step - auc: 0.5262 - loss: 0.6915 - val_auc: 0.4862 - val_loss: 0.6904 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 174ms/step - auc: 0.5572 - loss: 0.6824 - val_auc: 0.5075 - val_loss: 0.6872 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 176ms/step - auc: 0.5880 - loss: 0.6695 - val_auc: 0.5725 - val_loss: 0.6830 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 173ms/step - auc: 0.6447 - loss: 0.6489 - val_auc: 0.6060 - val_loss: 0.6800 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 172ms/step - auc: 0.6894 - loss: 0.6237 - val_auc: 0.6149 - val_loss: 0.6710 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 172ms/step - auc: 0.7323 - loss: 0.5956 - val_auc: 0.6300 - val_loss: 0.6967 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 173ms/step - auc: 0.7600 - loss: 0.5698 - val_auc: 0.6265 - val_loss: 0.6945 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 171ms/step - auc: 0.7947 - loss: 0.5384 - val_auc: 0.6015 - val_loss: 0.7204 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 167ms/step - auc: 0.8181 - loss: 0.5113 - val_auc: 0.6267 - val_loss: 0.7031 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 165ms/step - auc: 0.8454 - loss: 0.4791 - val_auc: 0.6042 - val_loss: 0.7602 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 166ms/step - auc: 0.8610 - loss: 0.4553 - val_auc: 0.6106 - val_loss: 0.7878 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 166ms/step - auc: 0.9010 - loss: 0.3897 - val_auc: 0.6058 - val_loss: 0.8781 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 165ms/step - auc: 0.9158 - loss: 0.3604 - val_auc: 0.6053 - val_loss: 0.9498 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 166ms/step - auc: 0.9271 - loss: 0.3356 - val_auc: 0.6076 - val_loss: 0.9706 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 166ms/step - auc: 0.9342 - loss: 0.3209 - val_auc: 0.6012 - val_loss: 1.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 167ms/step - auc: 0.9381 - loss: 0.3110 - val_auc: 0.6051 - val_loss: 1.0524 - learning_rate: 5.0000e-04\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step\n",
      "   ResNet ROC-AUC on train: 0.7902\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "   ResNet ROC-AUC: 0.6304\n",
      "\n",
      "Best Model: Neural Network (ROC-AUC: 0.6487)\n",
      "Target (>0.63): ACHIEVED\n"
     ]
    }
   ],
   "source": [
    "def train_models(X_train, y_train):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "    X_tr, y_tr = augment_features(X_tr, y_tr)\n",
    "    models = {}\n",
    "    results = {}\n",
    "\n",
    "    # # 1. Logistic Regression\n",
    "    # print(\"Training Logistic Regression...\")\n",
    "    # lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    # lr_model.fit(X_tr, y_tr)\n",
    "    # lr_pred = lr_model.predict(X_val)\n",
    "    # lr_score = roc_auc_score(y_val, lr_pred)\n",
    "    # models['Logistic Regression'] = lr_model\n",
    "    # results['Logistic Regression'] = lr_score\n",
    "    # print(f\"   Logistic Regression ROC-AUC: {lr_score:.4f}\")\n",
    "\n",
    "    # # 2. XGBoost\n",
    "    # print(\"Training XGBoost...\")\n",
    "    # xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "    # xgb_model.fit(X_tr, y_tr)\n",
    "    # xgb_pred = xgb_model.predict(X_val)\n",
    "    # xgb_score = roc_auc_score(y_val, xgb_pred)\n",
    "    # models['XGBoost'] = xgb_model\n",
    "    # results['XGBoost'] = xgb_score\n",
    "    # print(f\"   XGBoost ROC-AUC: {xgb_score:.4f}\")\n",
    "\n",
    "    # 3. Neural Network\n",
    "    print(\"Training Neural Network...\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        nn_model = keras.Sequential([\n",
    "            layers.Dense(4096, activation='relu', input_shape=(X_tr.shape[1],)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.6),\n",
    "\n",
    "            layers.Dense(2048, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.6),            \n",
    "\n",
    "            layers.Dense(1024, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            \n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.6),\n",
    "            \n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.6),\n",
    "            \n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.6),\n",
    "            \n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "    nn_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=[keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, min_lr=1e-6, mode='max')\n",
    "\n",
    "    nn_model.fit(\n",
    "        X_tr, y_tr, \n",
    "        epochs=100, \n",
    "        batch_size=128, \n",
    "        validation_data=(X_val, y_val), \n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    nn_train_pred = nn_model.predict(X_tr)\n",
    "    nn_train_score = roc_auc_score(y_tr, nn_train_pred)\n",
    "    print(f\"   Neural Network ROC-AUC on train: {nn_train_score:.4f}\")\n",
    "    \n",
    "    nn_pred = nn_model.predict(X_val)\n",
    "    nn_score = roc_auc_score(y_val, nn_pred)\n",
    "    models['Neural Network'] = nn_model\n",
    "    results['Neural Network'] = nn_score\n",
    "    print(f\"   Neural Network ROC-AUC: {nn_score:.4f}\")\n",
    "\n",
    "    nn_res_model, nn_res_score = res_net(X_tr, X_val, y_tr, y_val)\n",
    "    models['ResNet'] = nn_res_model\n",
    "    results['ResNet'] = nn_res_score\n",
    "\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    best_model = models[best_model_name]\n",
    "    best_score = results[best_model_name]\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name} (ROC-AUC: {best_score:.4f})\")\n",
    "    print(f\"Target (>0.63): {'ACHIEVED' if best_score > 0.63 else 'NOT ACHIEVED'}\")\n",
    "    \n",
    "    return best_model, best_score, best_model_name\n",
    "\n",
    "best_model, best_score, best_model_name = train_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66fd4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model on full dataset...\n",
      "Generating predictions...\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "✅ Submission saved! Shape: (7000, 2)\n",
      "Label distribution: 0    4024\n",
      "1    2976\n",
      "Name: count, dtype: int64\n",
      "   ID  Label\n",
      "0   0      0\n",
      "1   1      0\n",
      "2   2      0\n",
      "3   3      1\n",
      "4   4      1\n",
      "\n",
      "Submission format check:\n",
      "- Unique labels: [np.int64(0), np.int64(1)]\n",
      "- Should be: [0, 1] only\n"
     ]
    }
   ],
   "source": [
    "print(\"Training final model on full dataset...\")\n",
    "final_model = best_model\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "if 'Neural Network' in str(type(best_model)) or hasattr(best_model, 'predict') and not hasattr(best_model, 'predict_proba'):\n",
    "    test_probabilities = final_model.predict(X_test).flatten()\n",
    "    test_predictions = (test_probabilities > 0.5).astype(int)  \n",
    "else:\n",
    "    final_model.fit(X_train, y_train)\n",
    "    test_predictions = final_model.predict(X_test)  \n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],           \n",
    "    'Label': test_predictions      \n",
    "})\n",
    "\n",
    "submission.to_csv(f'data/submission_{best_model_name}.csv', index=False)\n",
    "print(f\"Submission saved! Shape: {submission.shape}\")\n",
    "print(f\"Label distribution: {pd.Series(test_predictions).value_counts().sort_index()}\")\n",
    "print(submission.head())\n",
    "\n",
    "print(f\"\\nSubmission format check:\")\n",
    "print(f\"- Unique labels: {sorted(submission['Label'].unique())}\")\n",
    "print(f\"- Should be: [0, 1] only\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
